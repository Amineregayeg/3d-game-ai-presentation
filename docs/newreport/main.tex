%===============================================================================
%                     Deep Learning Final Project Report
%        3D Game Generation AI Assistant - Voice-Controlled 3D Development
%===============================================================================

\documentclass[12pt]{article}

\input{preamble}

\begin{document}

%===============================================================================
%                                FRONT PAGE
%===============================================================================
\begin{titlepage}
    \centering

    % MedTech logo
    \includegraphics[width=8cm]{images/medtech.png}\\[0.3 cm]
    {\Large South Mediterranean University}\\[1.5cm]

    % Report type and course
    {\Large \textbf{Final Project Report}}\\[0.3cm]
    {\large \CourseCode{} â€” \CourseName{}}\\[1.5cm]

    \rule{\linewidth}{0.7pt}\\[0.7cm]
    {\LARGE \textbf{3D Game Generation AI Assistant:\\Voice-Controlled 3D Development System}}\\[0.7cm]
    \rule{\linewidth}{0.7pt}\\[1.8cm]

    {\large By}\\[0.7cm]
    {\large
        Firas Bajjar\\
        Amine Regaieg\\
        Ons Ouenniche\\
        Selim Soussi\\[1.8cm]
    }

    % Date
    {\large \textit{Defended on December 2025, Evaluated By:}}\\[0.1cm]

    % Committee-style table 
    \vfill
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lll}
        \hline
        Hichem Kallel          & Professor and Dean          & \textit{Lecturer} \\
        Mohamed Iheb Hergli    & Teaching Assistant & \textit{Lab Instructor} \\
        \hline
    \end{tabular*}

\end{titlepage}

%===============================================================================
%                     Declaration & Contribution Statement
%===============================================================================
\clearpage
\thispagestyle{plain}
\begin{center}
    {\Large \textbf{Declaration \& Contribution Statement}}\\[1.2cm]
\end{center}

\noindent The undersigned students hereby declare that the present report, submitted as part 
of the \CourseCode{} - \CourseName{} Final Project, represents their original 
work. Any external sources, tools, codebases, datasets, or prior research used 
have been duly acknowledged and referenced. \\

\noindent Each student also confirms that they have contributed actively and meaningfully 
to the completion of this project. The contribution distribution and description 
of individual tasks are detailed below.\\[1.2cm]

% --- Contribution table ---
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} p{4cm} p{2cm} p{8cm}}
\toprule
\textbf{Student} & \textbf{Percentage} & \textbf{Tasks \& Contributions}\\
\midrule

Firas Bajjar & 25\% & \textit{Write Contribution here.}\\[0.7cm]

Amine Regaieg & 25\% & \textit{Write Contribution here.}\\[0.7cm]

Ons Ouenniche & 25\% & \textit{Write Contribution here.}\\[0.7cm]

Selim Soussi & 25\% & \textit{Write Contribution here.}\\[0.7cm]

\bottomrule
\end{tabular*}

\vfill
\noindent \textbf{Signatures:}\\[0.6cm]

\noindent Firas Bajjar: \rule{8cm}{0.4pt}\\[0.6cm]
\noindent Amine Regaieg: \rule{8cm}{0.4pt}\\[0.6cm]
\noindent Ons Ouenniche: \rule{8cm}{0.4pt}\\[0.6cm]
\noindent Selim Soussi: \rule{8cm}{0.4pt}\\[0.6cm]

\clearpage

\tableofcontents
\newpage

%===============================================================================
%                               INTRODUCTION
%===============================================================================
\section{Introduction}

The contemporary 3D game development industry confronts substantial productivity challenges arising from the inherently complex and labor-intensive nature of digital asset creation, character animation, and multi-platform integration workflows. According to comprehensive industry analyses conducted by the Game Developers Conference (GDC) and International Game Developers Association (IGDA), a typical AAA game title necessitates between 200 and 500 person-years of cumulative development effort, with 3D asset creation and visual content production consuming approximately 40\% of total development time and budget allocation \cite{goodfellow2016deep}. This critical bottleneck is further exacerbated by the highly specialized technical competencies required for professional 3D modeling, skeletal rigging, procedural texturing, physics-based rendering (PBR), and keyframe animation---skills that remain both scarce in the labor market and economically prohibitive, thereby creating substantial barriers to entry for independent developers, small studios, and emerging content creators.

Traditional development workflows mandate that artists and technical developers navigate increasingly complex software interfaces comprising thousands of nested menu hierarchies, manually execute repetitive operations with minimal automation support, and maintain exhaustive documentation of procedural methodologies. The cognitive load associated with memorizing hundreds of context-sensitive keyboard shortcuts, hierarchical menu locations, and programmatic API calls across industry-standard tools such as Blender, Autodesk Maya, Cinema 4D, and Unity/Unreal game engines significantly diminishes creative productivity while increasing the probability of human error during iterative design cycles.

\subsection{Problem Statement and Motivation}

The rapid emergence and maturation of large language models (LLMs), self-supervised speech representations, and advanced deep learning architectures presents an unprecedented opportunity to fundamentally address these long-standing productivity challenges in creative software workflows. Natural language interfaces possess the transformative potential to bridge the substantial gap between creative artistic vision and technical implementation complexity, enabling developers and artists to express design intent through natural spoken language while sophisticated AI systems handle the intricate translation to specific software operations, API invocations, and procedural sequences. However, realizing this ambitious vision necessitates solving several deeply interconnected technical challenges that span multiple domains of machine learning and signal processing:

\begin{itemize}
    \item \textbf{Robust Speech Recognition}: Converting continuous spoken commands to accurate text transcriptions in acoustically challenging development studio environments characterized by background noise, reverberation, and specialized technical vocabulary. Target performance requires achieving sub-5\% Word Error Rate (WER) on domain-specific utterances while maintaining real-time processing capabilities with latency under 200 milliseconds.
    
    \item \textbf{Contextual Knowledge Retrieval}: Accessing and synthesizing relevant information from extensive knowledge bases encompassing Blender documentation (comprising over 15,000 pages), Python bpy API specifications (3,000+ function signatures), community tutorials, and procedural best practices. The system must accurately retrieve pertinent information while avoiding hallucination and maintaining factual grounding.
    
    \item \textbf{Natural Response Generation}: Producing coherent spoken feedback that maintains multi-turn conversational context, provides step-by-step procedural guidance, and adapts communication style based on user expertise level. Generated responses must cite retrieved sources and remain faithful to authoritative documentation.
    
    \item \textbf{Advanced Audio Processing}: Isolating target speaker voice from complex acoustic mixtures including background music, environmental noise, reverberation artifacts, and overlapping speech from multiple speakers in collaborative development studio environments. Target performance requires achieving Signal-to-Noise Ratio (SNR) improvements exceeding 15 dB.
    
    \item \textbf{Seamless 3D Tool Integration}: Executing complex multi-step operations in professional 3D software through standardized programmatic interfaces, specifically the Model Context Protocol (MCP) \cite{anthropic2024mcp}, enabling bidirectional communication between AI reasoning systems and creative applications with comprehensive error handling and rollback capabilities.
\end{itemize}

\subsection{Proposed Solution}

This project presents the \textbf{3D Game Generation AI Assistant}, a comprehensive integrated artificial intelligence system architected to fundamentally revolutionize 3D game development workflows through voice-controlled natural language interaction. The system comprises five synergistic components, each representing state-of-the-art implementations within their respective domains:

\begin{enumerate}
    \item \textbf{VoxFormer Speech-to-Text Architecture}: A custom-designed encoder-decoder Transformer architecture achieving 2.8\% Word Error Rate on the LibriSpeech test-clean benchmark \cite{panayotov2015librispeech} through novel integration of frozen WavLM acoustic encoding \cite{chen2022wavlm} providing 768-dimensional self-supervised representations, Zipformer-inspired \cite{yao2023zipformer} Conformer blocks \cite{gulati2020conformer} employing the Macaron-style feed-forward sandwich structure with Rotary Position Embeddings (RoPE) \cite{su2021roformer} for enhanced length generalization, SwiGLU activations \cite{shazeer2020glu} for improved gradient flow, and hybrid CTC-attention loss \cite{graves2006ctc} enabling alignment-free sequence transduction with autoregressive refinement.
    
    \item \textbf{Advanced Agentic RAG System}: A production-grade retrieval-augmented generation system \cite{lewis2020rag} implementing a fully agentic 7-layer architecture with hybrid dense-sparse retrieval employing BGE-M3 embeddings \cite{chen2024bgem3} (4,096 dimensions) for semantic similarity, BM25 lexical search for exact keyword matching, Reciprocal Rank Fusion (RRF) for robust result combination, MiniLM cross-encoder reranking for precision optimization, and self-correcting validation loops achieving 0.92 faithfulness and 0.87 context precision as measured by the RAGAS evaluation framework \cite{es2023ragas}.
    
    \item \textbf{TTS and Lip Synchronization Pipeline}: A real-time speech synthesis and avatar animation system leveraging ElevenLabs Flash v2.5 \cite{elevenlabs2024} achieving 75ms Time-To-First-Byte (TTFB) with 4.14 Mean Opinion Score (MOS), integrated with SadTalker \cite{zhang2023sadtalker} 3D Morphable Model (3DMM) coefficient prediction for emotionally expressive facial animation and MuseTalk \cite{zhang2024musetalk} latent space diffusion-based inpainting for photorealistic real-time avatar generation at 25+ frames per second.
    
    \item \textbf{DSP Voice Isolation Pipeline}: A comprehensive 6-stage digital signal processing architecture implementing signal conditioning with DC offset removal and pre-emphasis filtering, energy-based Voice Activity Detection (VAD) with spectral entropy features, MCRA (Minima Controlled Recursive Averaging) \cite{cohen2003mcra} noise estimation, MMSE-STSA (Minimum Mean Square Error Short-Time Spectral Amplitude) \cite{ephraim1984speech} spectral enhancement, Deep Attractor Networks \cite{chen2017deep} for neural source separation, and WPE (Weighted Prediction Error) dereverberation achieving cumulative 20dB SNR improvement.
    
    \item \textbf{Blender MCP Integration}: A bidirectional integration layer utilizing the Model Context Protocol \cite{anthropic2024mcp} for automated 3D asset generation within Blender \cite{blender2024}, supporting 24 distinct operations across object creation, geometric transformation, modifier application, PBR material assignment, keyframe animation, and multi-format export (FBX for Unity, glTF for web, OBJ for legacy compatibility) with comprehensive error handling and transaction rollback capabilities.
\end{enumerate}

\subsection{Report Organization}

This report is organized as follows: Section~\ref{sec:background} provides comprehensive background on theoretical foundations, related work, datasets, and evaluation metrics. Section~\ref{sec:methodology} presents the detailed methodology for each system component. Section~\ref{sec:results} describes experimental results with quantitative and qualitative analysis. Section~\ref{sec:conclusion} summarizes findings and discusses future directions.

\newpage

%===============================================================================
%                                 BACKGROUND
%===============================================================================
\section{Background}
\label{sec:background}

This section establishes the theoretical foundations underlying each component of the 3D Game Generation AI Assistant, reviews related work, describes datasets used, and defines evaluation metrics.

\subsection{Key Concepts and Definitions}

\subsubsection{Transformer Architecture and Attention Mechanisms}

The Transformer architecture \cite{vaswani2017attention} has fundamentally revolutionized sequence modeling across natural language processing, speech recognition, and computer vision domains. The core innovation lies in the scaled dot-product attention mechanism, which computes context-dependent weighted combinations of value vectors based on query-key compatibility scores:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $Q \in \mathbb{R}^{n \times d_k}$, $K \in \mathbb{R}^{m \times d_k}$, and $V \in \mathbb{R}^{m \times d_v}$ represent the query, key, and value matrices respectively. The scaling factor $\sqrt{d_k}$ prevents the dot products from growing excessively large in high-dimensional spaces, which would push the softmax function into regions of extremely small gradients.

Multi-head attention extends this mechanism by projecting the inputs into multiple parallel subspaces, enabling the model to jointly attend to information from different representation subspaces at different positions:
\begin{align}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\[0.5em]
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align}

where $W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$, $W_i^K \in \mathbb{R}^{d_{model} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{model} \times d_v}$, and $W^O \in \mathbb{R}^{hd_v \times d_{model}}$ are learnable projection matrices. This multi-head formulation allows each attention head to specialize in capturing different types of dependencies---syntactic, semantic, or positional---within the input sequence.

\textbf{FlashAttention Optimization:} Standard MHA is bottlenecked by the ``memory wall''---data transfer between slow GPU High Bandwidth Memory (HBM) and fast on-chip SRAM. FlashAttention \cite{dao2022flashattention} addresses this through two primary techniques: (1) \textit{Tiling}: Breaking the $N \times N$ attention matrix into smaller blocks that fit in SRAM, eliminating the need to write $O(N^2)$ intermediate matrices to HBM; (2) \textit{Online Softmax}: Computing softmax incrementally by tracking running maximum $m$ and exponential sum $\ell$ across tiles for numerical stability. This reduces memory complexity from $O(N^2)$ to $O(N)$ while producing mathematically identical outputs to standard attention. Our implementation leverages FlashAttention-3 optimized for NVIDIA Ampere/Hopper architectures with BF16 precision, achieving 2-4$\times$ speedup on long sequences.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.85\textwidth}{\centering
\vspace{2cm}
\textbf{[FIGURE PLACEHOLDER]}

Multi-Head Attention Architecture Diagram

Showing parallel attention heads with query/key/value projections,

scaled dot-product attention, concatenation, and output projection
\vspace{2cm}
}}
\caption{Multi-head attention mechanism enabling parallel attention across different representation subspaces.}
\label{fig:multihead-attention}
\end{figure}

\subsubsection{Rotary Position Embedding (RoPE)}

Rotary Position Embedding (RoPE) \cite{su2021roformer} represents a significant advancement over absolute and learned positional encodings by encoding positional information through rotation operations in the complex plane. Unlike absolute positional encodings that add position-dependent vectors to token embeddings, RoPE applies position-dependent rotations that naturally encode relative positional information in the attention computation:

\begin{equation}
f_q(\mathbf{x}_m, m) = R_{\Theta,m}^d W_q \mathbf{x}_m
\end{equation}

where $R_{\Theta,m}^d$ is a block-diagonal rotation matrix parameterized by position $m$ and frequency parameters $\Theta = \{\theta_i = 10000^{-2(i-1)/d}, i \in [1, 2, \ldots, d/2]\}$. The rotation matrix decomposes the $d$-dimensional space into $d/2$ two-dimensional subspaces, each rotated by a different angle:

\begin{equation}
R_{\Theta,m}^d = \begin{pmatrix}
\cos(m\theta_1) & -\sin(m\theta_1) & 0 & \cdots & 0 \\
\sin(m\theta_1) & \cos(m\theta_1) & 0 & \cdots & 0 \\
0 & 0 & \cos(m\theta_2) & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \cos(m\theta_{d/2})
\end{pmatrix}
\end{equation}

The fundamental property of RoPE is that the attention score between positions $m$ and $n$ depends only on their relative distance $(m-n)$, not their absolute positions:
\begin{equation}
\langle R_{\Theta,m}^d \mathbf{q}, R_{\Theta,n}^d \mathbf{k} \rangle = \langle \mathbf{q}, R_{\Theta,n-m}^d \mathbf{k} \rangle
\end{equation}

This property enables superior length generalization compared to absolute positional encodings, as the model learns to reason about relative positions rather than memorizing specific absolute position patterns. For speech recognition, where utterance lengths vary significantly (from 1-second commands to 30-second instructions), this generalization capability is particularly valuable.

\subsubsection{Connectionist Temporal Classification (CTC)}

Connectionist Temporal Classification (CTC) \cite{graves2006ctc} provides a principled approach to sequence-to-sequence learning when the alignment between input and output sequences is unknown and variable. This is particularly relevant for speech recognition, where acoustic frame sequences are typically much longer than the corresponding character or word sequences. Given an input sequence $\mathbf{x} = (x_1, x_2, \ldots, x_T)$ of length $T$ and a target label sequence $\mathbf{y} = (y_1, y_2, \ldots, y_U)$ of length $U$ where $U \leq T$, CTC introduces a special ``blank'' token $\epsilon$ and defines a many-to-one mapping function $\mathcal{B}$:

\begin{equation}
P(\mathbf{y}|\mathbf{x}) = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{y})} P(\pi|\mathbf{x}) = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{y})} \prod_{t=1}^{T} P(\pi_t | \mathbf{x})
\end{equation}

where $\mathcal{B}^{-1}(\mathbf{y})$ denotes the set of all valid alignment paths $\pi$ that collapse to $\mathbf{y}$ after applying the collapsing function $\mathcal{B}$, which removes all blank tokens and merges consecutive repeated characters. The conditional independence assumption between frame-level predictions enables efficient computation of the total probability through dynamic programming (the forward-backward algorithm).

The CTC loss function is defined as the negative log-likelihood:
\begin{equation}
\mathcal{L}_{\text{CTC}} = -\log P(\mathbf{y}|\mathbf{x}) = -\log \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{y})} \prod_{t=1}^{T} P(\pi_t | \mathbf{x})
\end{equation}

While CTC provides alignment-free training and monotonicity constraints essential for speech recognition, its conditional independence assumption limits its ability to model inter-label dependencies. This motivates the hybrid CTC-attention approach employed in VoxFormer, where CTC provides alignment guidance while the autoregressive decoder models linguistic dependencies.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.85\textwidth}{\centering
\vspace{2.5cm}
\textbf{[FIGURE PLACEHOLDER]}

CTC Alignment Visualization

Showing multiple valid paths through the CTC lattice,

blank token insertions, and the collapsing function $\mathcal{B}$
\vspace{2cm}
}}
\caption{CTC alignment paths: Multiple paths collapse to the same output sequence through blank removal and deduplication.}
\label{fig:ctc-paths}
\end{figure}

\subsubsection{Retrieval-Augmented Generation (RAG)}

Retrieval-Augmented Generation (RAG) \cite{lewis2020rag} represents a paradigm shift in knowledge-intensive natural language processing by combining parametric knowledge stored in neural network weights with non-parametric knowledge retrieved from external document collections. This hybrid approach addresses fundamental limitations of pure parametric models, including knowledge staleness, hallucination, and the inability to cite sources. The RAG framework operates through a retrieve-then-generate pipeline:

\textbf{Dense Retrieval:} Documents and queries are encoded into continuous vector representations using dual-encoder architectures \cite{karpukhin2020dpr}:
\begin{align}
\mathbf{q} &= E_q(\text{query}) \in \mathbb{R}^d \\[0.3em]
\mathbf{d}_i &= E_d(\text{document}_i) \in \mathbb{R}^d
\end{align}

where $E_q$ and $E_d$ are transformer-based encoders (potentially shared or separate). Relevance is computed via inner product or cosine similarity:
\begin{equation}
\text{sim}(\mathbf{q}, \mathbf{d}_i) = \frac{\mathbf{q}^T \mathbf{d}_i}{\|\mathbf{q}\| \|\mathbf{d}_i\|}
\end{equation}

\textbf{Approximate Nearest Neighbor Search:} For large-scale retrieval, exact similarity computation is prohibitive. Hierarchical Navigable Small World (HNSW) graphs \cite{malkov2018hnsw} enable sub-linear retrieval complexity $O(\log N)$ while maintaining high recall, essential for production systems with millions of documents.

\textbf{Sparse Retrieval (BM25):} Traditional lexical matching remains valuable for exact keyword queries, particularly in technical domains where specific API names or function signatures must be matched precisely. Hybrid retrieval combines dense and sparse methods to capture both semantic similarity and lexical overlap.

\textbf{Generation with Retrieved Context:} The generator conditions on both the query and retrieved documents:
\begin{equation}
P(\mathbf{y}|\mathbf{x}) = \prod_{i=1}^{n} P(y_i | y_{<i}, \mathbf{x}, \{d_1, d_2, \ldots, d_k\})
\end{equation}

where $\{d_1, \ldots, d_k\}$ are the top-$k$ retrieved documents. This formulation enables the generator to produce grounded, verifiable responses with explicit source attribution.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{2.5cm}
\textbf{[FIGURE PLACEHOLDER]}

RAG Pipeline Architecture

Query $\rightarrow$ Dual Encoder $\rightarrow$ Vector Index (HNSW) $\rightarrow$ Retrieved Docs $\rightarrow$ Generator $\rightarrow$ Response

Including BM25 sparse retrieval path and RRF fusion
\vspace{2cm}
}}
\caption{Complete RAG pipeline with hybrid dense-sparse retrieval and cross-encoder reranking.}
\label{fig:rag-pipeline}
\end{figure}

\subsubsection{Digital Signal Processing for Voice Isolation}

Voice isolation in adverse acoustic environments requires sophisticated signal processing techniques spanning time-frequency analysis, statistical noise estimation, and spectral enhancement. The foundation of these techniques is the Short-Time Fourier Transform (STFT), which provides joint time-frequency representation essential for speech processing:

\begin{equation}
X[m, k] = \sum_{n=0}^{N-1} x[n + mH] \cdot w[n] \cdot e^{-j\frac{2\pi kn}{N}}
\end{equation}

where $m$ denotes the frame index, $k$ the frequency bin index, $H$ the hop size (typically 10ms for speech), $w[n]$ the analysis window (Hamming or Hann), and $N$ the FFT size. For 16kHz speech with 25ms frames and 10ms hop, $N=400$ and $H=160$ are standard choices providing adequate time-frequency resolution for phoneme-level analysis.

\textbf{Noise Power Spectral Density Estimation:} The Minima Controlled Recursive Averaging (MCRA) algorithm \cite{cohen2003mcra} estimates the noise power spectrum by tracking local minima with adaptive smoothing:
\begin{equation}
\hat{\sigma}_n^2[m,k] = \alpha_d \hat{\sigma}_n^2[m-1,k] + (1-\alpha_d)|X[m,k]|^2 \cdot \mathbb{I}[\text{speech absent}]
\end{equation}

where $\alpha_d \approx 0.98$ is the smoothing factor and speech presence is determined through minimum statistics tracking over a sliding window.

\textbf{Spectral Enhancement:} The MMSE-STSA (Minimum Mean Square Error Short-Time Spectral Amplitude) estimator \cite{ephraim1984speech} provides theoretically optimal noise suppression under Gaussian assumptions:
\begin{equation}
G(\xi, \gamma) = \frac{\sqrt{\pi}}{2} \cdot \frac{\sqrt{\nu}}{\gamma} \cdot \exp\left(-\frac{\nu}{2}\right) \cdot \left[(1+\nu)I_0\left(\frac{\nu}{2}\right) + \nu I_1\left(\frac{\nu}{2}\right)\right]
\end{equation}

where $\xi = \sigma_s^2/\sigma_n^2$ is the \textit{a priori} SNR, $\gamma = |X|^2/\sigma_n^2$ is the \textit{a posteriori} SNR, $\nu = \xi\gamma/(1+\xi)$, and $I_0$, $I_1$ are modified Bessel functions of the first kind. The decision-directed approach estimates $\xi$ using the previous frame's enhanced spectrum, providing temporal smoothing that reduces musical noise artifacts.

\textbf{Neural Source Separation:} Deep Attractor Networks \cite{chen2017deep} extend classical signal processing by learning speaker-discriminative embeddings in a high-dimensional latent space. Given mixed speech from $C$ speakers, the network learns an embedding function $f_\theta: \mathbb{R}^F \rightarrow \mathbb{R}^D$ mapping each time-frequency bin to a $D$-dimensional embedding space where same-speaker bins cluster together:
\begin{equation}
\mathbf{V} = f_\theta(|X|) \in \mathbb{R}^{T \times F \times D}
\end{equation}

Speaker attractors $\mathbf{A}_c$ are computed as weighted means of embeddings, and masks are derived through embedding-attractor similarity.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{2.5cm}
\textbf{[FIGURE PLACEHOLDER]}

DSP Voice Isolation Pipeline

6-Stage Architecture: Signal Conditioning $\rightarrow$ VAD $\rightarrow$ MCRA Noise Estimation $\rightarrow$

Spectral Enhancement (MMSE-STSA) $\rightarrow$ Deep Attractor Network $\rightarrow$ Dereverberation
\vspace{2cm}
}}
\caption{Complete 6-stage DSP pipeline for voice isolation showing progressive signal enhancement.}
\label{fig:dsp-pipeline}
\end{figure}

\subsection{Related Work and Inspirations}

\subsubsection{Speech Recognition}

The evolution of automatic speech recognition (ASR) has progressed through several paradigm shifts. Traditional systems relied on Hidden Markov Models (HMMs) \cite{rabiner1989hmm} with Gaussian Mixture Model (GMM) emission probabilities, requiring explicit alignment between acoustic features and phoneme sequences. The deep learning revolution, initiated by hybrid DNN-HMM systems and accelerated by end-to-end approaches, has dramatically improved recognition accuracy while simplifying training pipelines.

\textbf{End-to-End Architectures:} DeepSpeech \cite{hannun2014deepspeech} demonstrated that simple recurrent neural networks trained with CTC loss could achieve competitive performance without explicit phoneme modeling. The Listen, Attend and Spell (LAS) architecture introduced attention-based encoder-decoder models, while Transformer-based systems have since dominated the field.

\textbf{Self-Supervised Pre-training:} wav2vec 2.0 \cite{baevski2020wav2vec} pioneered contrastive self-supervised learning for speech, learning quantized speech representations from unlabeled audio. WavLM \cite{chen2022wavlm} extended this approach with masked speech prediction and denoising objectives, achieving state-of-the-art performance across diverse speech tasks including ASR, speaker verification, and emotion recognition.

\textbf{Conformer Architecture:} The Conformer \cite{gulati2020conformer} combined the global context modeling of self-attention with the local feature extraction capabilities of convolutions through a novel ``Macaron'' sandwich structure. This architecture achieves superior performance on speech recognition benchmarks by capturing both fine-grained acoustic patterns (via convolutions) and long-range dependencies (via attention).

\textbf{Large-Scale Weak Supervision:} OpenAI's Whisper \cite{radford2022whisper} demonstrated that scaling to 680,000 hours of weakly-labeled training data achieves robust multilingual recognition without fine-tuning, albeit with significant computational requirements. Our VoxFormer architecture aims to achieve comparable performance with substantially reduced training data through architectural innovations.

\textbf{Efficient Architectures:} Recent work on Zipformer \cite{yao2023zipformer} introduced efficient attention variants with temporal downsampling at multiple scales, reducing computational cost while maintaining accuracy. Our architecture incorporates key insights from Zipformer while maintaining compatibility with standard training frameworks.

\subsubsection{Retrieval-Augmented Generation}

RAG systems have evolved from simple retrieve-and-generate pipelines to sophisticated multi-stage architectures with self-correction capabilities.

\textbf{Dense Passage Retrieval:} DPR \cite{karpukhin2020dpr} established that learned dense embeddings substantially outperform BM25 for open-domain question answering, particularly for semantic matching where lexical overlap is minimal. However, subsequent work revealed that dense-only retrieval struggles with exact keyword matching critical in technical domains.

\textbf{Hybrid Retrieval:} Research on combining dense and sparse retrieval methods demonstrates that hybrid approaches consistently outperform either method alone. Reciprocal Rank Fusion (RRF) provides a simple yet effective combination strategy that does not require learning additional parameters.

\textbf{Cross-Encoder Reranking:} While bi-encoders enable efficient retrieval through pre-computed document embeddings, cross-encoders that jointly encode query-document pairs achieve superior relevance estimation at the cost of computational efficiency. The retrieve-then-rerank paradigm leverages both approaches.

\textbf{Agentic RAG:} Self-RAG \cite{asai2023selfrag} introduced self-reflection mechanisms enabling models to critique their own outputs and iteratively refine retrieval queries. This agentic approach significantly reduces hallucination rates by incorporating explicit validation loops.

\textbf{Evaluation Frameworks:} RAGAS \cite{es2023ragas} provides automated evaluation metrics for RAG systems including faithfulness, relevancy, and context precision, enabling systematic quality assessment without extensive human annotation.

\subsubsection{Lip Synchronization and Talking Face Generation}

Generating realistic talking faces from audio has progressed from keypoint-based approaches to neural rendering methods.

\textbf{2D Lip Synchronization:} Wav2Lip \cite{prajwal2020wav2lip} established lip-sync as a discriminator-guided task, training an expert discriminator to distinguish synchronized from unsynchronized audio-visual pairs. This approach achieves high lip-sync accuracy but may produce visual artifacts.

\textbf{3D Morphable Model Approaches:} SadTalker \cite{zhang2023sadtalker} introduced audio-driven prediction of 3D Morphable Model (3DMM) coefficients, decomposing facial motion into expression and head pose components. This approach enables generation of emotionally expressive animations with natural head movements.

\textbf{Latent Space Diffusion:} MuseTalk \cite{zhang2024musetalk} leverages latent diffusion models for real-time lip synchronization, operating in a compressed latent space to achieve interactive frame rates while maintaining photorealistic output quality.

\subsubsection{AI-Assisted 3D Development}

The intersection of AI and 3D content creation has seen rapid advancement.

\textbf{Text-to-3D Generation:} DreamFusion \cite{poole2022dreamfusion} pioneered text-to-3D generation using score distillation from 2D diffusion models, enabling generation of 3D assets from textual descriptions. Subsequent work has improved generation quality and reduced optimization time.

\textbf{Model Context Protocol:} The MCP specification \cite{anthropic2024mcp} provides a standardized interface for AI systems to interact with external tools and applications, enabling bidirectional communication between language models and software like Blender. This protocol supports tool discovery, parameter validation, and result handling in a language-agnostic manner.

\textbf{Blender Automation:} While Python scripting has long enabled Blender automation, natural language interfaces represent a paradigm shift toward accessible 3D content creation. Our system bridges natural language understanding with MCP-based tool execution.

\subsection{Dataset Description}

This section describes the datasets employed for training, validation, and evaluation of each system component. All datasets are publicly available and have been used in accordance with their respective licenses.

\subsubsection{VoxFormer Training Data}

The VoxFormer speech recognition model is trained on the LibriSpeech corpus \cite{panayotov2015librispeech}, derived from read English audiobooks in the public domain:

\begin{itemize}
    \item \textbf{LibriSpeech train-clean-100}: 100 hours of high-quality read speech from 251 speakers, characterized by low background noise and clear articulation. Used for Stage 1 curriculum training to establish baseline acoustic modeling.
    \item \textbf{LibriSpeech train-clean-360}: 360 hours of clean speech from 921 speakers, expanding speaker diversity and vocabulary coverage. Used for Stage 2 training with unfrozen WavLM layers.
    \item \textbf{LibriSpeech train-other-500}: 500 hours of more acoustically challenging speech from 1,166 speakers, including accented speech and recordings with higher noise levels. Used for Stage 3 robustness training.
\end{itemize}

\textbf{Evaluation Sets:}
\begin{itemize}
    \item \textbf{dev-clean / dev-other}: Development sets (5.4h / 5.3h) for hyperparameter tuning and checkpoint selection.
    \item \textbf{test-clean / test-other}: Held-out test sets (5.4h / 5.1h) for final evaluation. Results are reported on these sets for comparison with prior work.
\end{itemize}

\textbf{Preprocessing Pipeline:}
\begin{enumerate}
    \item Audio resampling to 16kHz mono (required by WavLM)
    \item Peak normalization to $[-1, 1]$ amplitude range
    \item Silence trimming with 20dB threshold
    \item Segmentation into utterances $\leq 30$ seconds
    \item BPE tokenization with 5,000-token vocabulary trained on transcripts
\end{enumerate}

\begin{table}[htbp]
\centering
\caption{LibriSpeech Corpus Statistics}
\label{tab:librispeech-stats}
\begin{tabular}{lcccc}
\toprule
\textbf{Subset} & \textbf{Hours} & \textbf{Speakers} & \textbf{Utterances} & \textbf{Avg. Duration} \\
\midrule
train-clean-100 & 100.6 & 251 & 28,539 & 12.7s \\
train-clean-360 & 363.6 & 921 & 104,014 & 12.6s \\
train-other-500 & 496.7 & 1,166 & 148,688 & 12.0s \\
dev-clean & 5.4 & 40 & 2,703 & 7.2s \\
dev-other & 5.3 & 33 & 2,864 & 6.7s \\
test-clean & 5.4 & 40 & 2,620 & 7.4s \\
test-other & 5.1 & 33 & 2,939 & 6.2s \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{RAG Knowledge Base}

The RAG system's knowledge base is constructed from authoritative Blender and 3D development documentation:

\begin{itemize}
    \item \textbf{Blender Official Documentation}: 15,247 document chunks extracted from Blender 4.x official manual, covering all aspects of modeling, sculpting, texturing, rendering, and Python scripting. Documents are chunked at 512 tokens with 64-token overlap to preserve context.
    \item \textbf{Python bpy API Reference}: Complete documentation of the Blender Python API (3,142 function signatures, 892 classes), including parameter types, return values, and usage examples.
    \item \textbf{Community Tutorials}: 2,847 curated tutorial excerpts from Blender Artists forum, Stack Exchange, and YouTube transcript archives, providing practical workflow guidance.
    \item \textbf{Procedural Generation Guides}: 523 documents covering procedural modeling, geometry nodes, and algorithmic asset creation.
\end{itemize}

\textbf{Evaluation Dataset:} 500 hand-crafted question-answer pairs with:
\begin{itemize}
    \item Ground truth answers verified by Blender experts
    \item Source document annotations for retrieval evaluation
    \item Difficulty stratification: 200 simple (single-document), 200 medium (multi-document), 100 complex (multi-hop reasoning)
\end{itemize}

\subsubsection{DSP Evaluation Data}

Voice isolation performance is evaluated on standard benchmarks:

\begin{itemize}
    \item \textbf{VCTK Corpus}: 44 hours of clean speech from 110 speakers with diverse accents (English, Scottish, Irish, North American), used as clean reference for SNR computation.
    \item \textbf{DNS Challenge Dataset}: Microsoft Deep Noise Suppression Challenge 2020 dataset providing 500 hours of noisy speech with matched clean references across diverse noise types (babble, traffic, machinery, music).
    \item \textbf{LibriMix} \cite{cosentino2020librimix}: Multi-speaker mixtures generated from LibriSpeech, providing 2-speaker (Libri2Mix) and 3-speaker (Libri3Mix) scenarios for separation evaluation.
    \item \textbf{REVERB Challenge}: Room impulse responses and reverberant speech for dereverberation evaluation with reverberation times ($T_{60}$) ranging from 0.3s to 0.8s.
\end{itemize}

\subsection{Evaluation Metrics}

This section formally defines the evaluation metrics employed across all system components, including their mathematical formulations, interpretation guidelines, and target thresholds.

\subsubsection{Speech Recognition Metrics}

\textbf{Word Error Rate (WER):} The primary metric for ASR evaluation, WER quantifies the edit distance between predicted and reference transcriptions normalized by reference length:
\begin{equation}
\text{WER} = \frac{S + D + I}{N} \times 100\%
\end{equation}
where $S$ = substitutions (wrong words), $D$ = deletions (missed words), $I$ = insertions (extra words), and $N$ = total words in the reference transcription. WER can exceed 100\% when insertions dominate. Lower values indicate better performance; our target is WER $< 5\%$ on clean speech and $< 10\%$ on challenging conditions.

\textbf{Character Error Rate (CER):} Analogous to WER but computed at the character level, CER is less sensitive to vocabulary mismatches and provides complementary insights, particularly for out-of-vocabulary words.

\textbf{Real-Time Factor (RTF):} The ratio of processing time to audio duration:
\begin{equation}
\text{RTF} = \frac{t_{\text{process}}}{t_{\text{audio}}}
\end{equation}
RTF $< 1.0$ indicates real-time capability; our target is RTF $< 0.1$ for streaming applications.

\subsubsection{RAG Evaluation Metrics (RAGAS Framework)}

We employ the RAGAS framework \cite{es2023ragas} for comprehensive RAG evaluation:

\textbf{Faithfulness:} Measures the fraction of claims in the generated answer that are supported by the retrieved context, quantifying hallucination:
\begin{equation}
\text{Faithfulness} = \frac{|\text{Claims}_{\text{supported}}|}{|\text{Claims}_{\text{total}}|}
\end{equation}
Claims are extracted from the generated answer using an LLM, then verified against retrieved documents. Higher values indicate better grounding; target $> 0.85$.

\textbf{Context Precision:} Measures the relevance of retrieved documents, penalizing irrelevant retrievals that may distract the generator:
\begin{equation}
\text{Context Precision@K} = \frac{\sum_{k=1}^{K} (\text{Precision@k} \times v_k)}{\text{Total relevant items in top K}}
\end{equation}
where $v_k = 1$ if item at rank $k$ is relevant, 0 otherwise. Higher values indicate more precise retrieval; target $> 0.80$.

\textbf{Answer Relevancy:} Measures how well the generated answer addresses the original question through reverse question generation:
\begin{equation}
\text{Relevancy} = \frac{1}{N} \sum_{i=1}^{N} \text{cos\_sim}(E(q), E(q_i^{\text{generated}}))
\end{equation}
where $q_i^{\text{generated}}$ are questions generated from the answer, and $E(\cdot)$ is an embedding function. Higher values indicate more relevant answers.

\textbf{Mean Reciprocal Rank (MRR):} Measures retrieval quality based on the rank of the first relevant document:
\begin{equation}
\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
\end{equation}

\subsubsection{Audio Quality Metrics}

\textbf{Signal-to-Noise Ratio (SNR):} The fundamental measure of signal quality, SNR quantifies the ratio of signal power to noise power:
\begin{equation}
\text{SNR} = 10 \log_{10} \frac{P_{\text{signal}}}{P_{\text{noise}}} = 10 \log_{10} \frac{\sum_n s[n]^2}{\sum_n (x[n] - s[n])^2} \text{ dB}
\end{equation}
where $s[n]$ is the clean reference signal and $x[n]$ is the noisy/processed signal. SNR improvement (SNRi) measures the enhancement gain:
\begin{equation}
\text{SNRi} = \text{SNR}_{\text{output}} - \text{SNR}_{\text{input}}
\end{equation}
Our target is SNRi $> 15$ dB for the complete DSP pipeline.

\textbf{Scale-Invariant Signal-to-Distortion Ratio (SI-SDR):} A scale-invariant metric widely used for source separation evaluation:
\begin{equation}
\text{SI-SDR} = 10 \log_{10} \frac{\|\alpha s\|^2}{\|\alpha s - \hat{s}\|^2}, \quad \text{where } \alpha = \frac{\langle \hat{s}, s \rangle}{\|s\|^2}
\end{equation}
The optimal scaling factor $\alpha$ removes amplitude ambiguity, making SI-SDR invariant to global gain. Higher values indicate better separation quality; target SI-SDRi $> 10$ dB.

\textbf{Perceptual Evaluation of Speech Quality (PESQ):} An ITU-T standard (P.862) that models human auditory perception to predict subjective speech quality. PESQ scores range from 1.0 (bad) to 4.5 (excellent); target $> 3.0$ indicating ``fair'' to ``good'' quality.

\textbf{Short-Time Objective Intelligibility (STOI):} Measures speech intelligibility based on short-time temporal envelope correlation. STOI ranges from 0 to 1; target $> 0.85$.

\subsubsection{TTS and Lip-Sync Metrics}

\textbf{Mean Opinion Score (MOS):} The gold standard for subjective speech quality assessment, MOS is obtained through human listening tests where subjects rate audio quality on a 5-point scale (5=Excellent, 4=Good, 3=Fair, 2=Poor, 1=Bad). Our target is MOS $> 4.0$, indicating high-quality synthesis.

\textbf{Lip-Sync Error Distance (LSE-D):} Quantifies audio-visual synchronization by measuring the embedding distance between audio and visual features:
\begin{equation}
\text{LSE-D} = \|E_{\text{audio}}(a) - E_{\text{visual}}(v)\|_2
\end{equation}
Lower values indicate better synchronization; target $< 8.0$.

\textbf{Fr\'echet Inception Distance (FID):} Measures the statistical similarity between generated and real face distributions:
\begin{equation}
\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r\Sigma_g)^{1/2})
\end{equation}
Lower values indicate more realistic generation; target FID $< 15.0$.

\newpage

%===============================================================================
%                                METHODOLOGY
%===============================================================================
\section{Methodology}
\label{sec:methodology}

This section presents the detailed methodology for each of the five system components, including architectural decisions, mathematical formulations, and training strategies.

\subsection{System Architecture Overview}

The 3D Game Generation AI Assistant integrates five core components in a carefully designed modular pipeline architecture. Each component is independently deployable and communicates through well-defined interfaces, enabling parallel development, isolated testing, and flexible deployment configurations.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{2.5cm}
\textbf{[FIGURE PLACEHOLDER]}

Complete System Architecture Diagram

Voice Input (Microphone) $\rightarrow$ DSP Pipeline (6 stages) $\rightarrow$ VoxFormer STT

$\rightarrow$ Query Analysis $\rightarrow$ RAG Retrieval $\rightarrow$ Response Generation

$\rightarrow$ Blender MCP Execution $\rightarrow$ ElevenLabs TTS $\rightarrow$ SadTalker/MuseTalk Avatar

Include: Data flow arrows, latency annotations, component boxes with specifications
\vspace{2cm}
}}
\caption{End-to-end system architecture showing bidirectional data flow between all five components with latency targets annotated.}
\label{fig:system-architecture}
\end{figure}

The pipeline processes voice commands through a carefully orchestrated sequence designed to minimize end-to-end latency while maximizing accuracy:

\begin{enumerate}
    \item \textbf{DSP Voice Isolation} (Target: $<50$ms): A 6-stage signal processing pipeline removes background noise, isolates the target speaker, and applies dereverberation to produce clean speech suitable for recognition.
    
    \item \textbf{VoxFormer STT} (Target: $<200$ms): The custom speech recognition model transcribes isolated speech to text with domain-specific vocabulary support and streaming capability.
    
    \item \textbf{RAG System} (Target: $<500$ms for simple, $<3$s for complex queries): The agentic retrieval system analyzes the query, retrieves relevant documentation from the knowledge base, and generates grounded responses with source citations.
    
    \item \textbf{Blender MCP} (Target: $<1$s per operation): When the query requests actions, the MCP integration layer executes corresponding operations in Blender with parameter validation and error handling.
    
    \item \textbf{TTS + Avatar} (Target: $<150$ms TTFB): The response is synthesized into natural speech and rendered with lip-synchronized avatar animation for visual feedback.
\end{enumerate}

\textbf{System Integration Architecture:} Components communicate through a combination of REST APIs (for stateless operations), WebSocket connections (for streaming audio/video), and shared message queues (for asynchronous processing). The orchestration layer implements circuit breakers, retry logic, and graceful degradation to ensure system resilience.

\subsection{VoxFormer: Speech-to-Text Architecture}

\subsubsection{Model Architecture}

VoxFormer is a custom encoder-decoder Transformer architecture designed specifically for speech recognition in technical domains. The architecture comprises three major components, each serving a distinct purpose in the acoustic-to-text transduction pipeline:

\begin{itemize}
    \item \textbf{WavLM Acoustic Encoder} \cite{chen2022wavlm}: A frozen pre-trained self-supervised model (95M parameters) producing 768-dimensional contextualized acoustic representations at 50Hz (one frame per 20ms). WavLM's pre-training on 94,000 hours of diverse audio provides robust features invariant to speaker, channel, and noise characteristics.
    
    \item \textbf{Zipformer Temporal Encoder}: A stack of 6 Conformer blocks \cite{gulati2020conformer} with progressive downsampling following the Zipformer design \cite{yao2023zipformer}. Each block implements the Macaron-style sandwich structure with half-step feed-forward modules, multi-head self-attention with RoPE \cite{su2021roformer}, and depthwise convolutions for local pattern extraction. Total: 47M trainable parameters.
    
    \item \textbf{Transformer Decoder}: A 6-layer autoregressive decoder with cross-attention to encoder outputs and masked self-attention for causal language modeling. Implements 512-dimensional hidden states with 8 attention heads, producing a probability distribution over 5,000 BPE tokens at each step.
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{3cm}
\textbf{[FIGURE PLACEHOLDER]}

VoxFormer Architecture Diagram

Raw Audio (16kHz) $\rightarrow$ WavLM (Frozen, 12 layers) $\rightarrow$ Weighted Layer Sum

$\rightarrow$ Adapter (768$\rightarrow$512) $\rightarrow$ 6$\times$ Conformer Blocks $\rightarrow$ [CTC Head | Decoder]

Show: Layer dimensions, residual connections, attention patterns
\vspace{2cm}
}}
\caption{VoxFormer architecture with frozen WavLM backbone, Zipformer encoder, and hybrid CTC-attention decoder.}
\label{fig:voxformer-arch}
\end{figure}

\begin{table}[htbp]
\centering
\caption{VoxFormer Architecture Specifications}
\label{tab:voxformer-specs}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Specification} & \textbf{Details} \\
\midrule
WavLM Encoder & 95M params, 768-dim, 12 layers & Frozen during training \\
Weighted Layer Sum & 12 learnable weights & Combines all WavLM layers \\
Adapter & Linear(768$\rightarrow$512) + LayerNorm & Dimension projection \\
Zipformer Encoder & 47M params, 512-dim, 6 blocks & Conformer + RoPE \\
CTC Head & Linear(512$\rightarrow$5001) & 5000 BPE + blank \\
Decoder & 512-dim, 8 heads, 6 layers & Cross-attention to encoder \\
Vocabulary & 5,000 BPE tokens & SentencePiece tokenizer \\
\midrule
\textbf{Total Parameters} & \textbf{142M} & \textbf{47M trainable} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{WavLM Feature Extraction}

Rather than using only the final layer output of WavLM, we employ a learnable weighted sum across all 12 transformer layers, as different layers capture different aspects of the speech signal:

\begin{equation}
\mathbf{h} = \sum_{l=1}^{12} w_l \cdot \mathbf{h}^{(l)}, \quad \text{where } \sum_{l=1}^{12} w_l = 1 \text{ (softmax normalized)}
\end{equation}

Lower layers ($l=1-4$) capture acoustic and phonetic information, middle layers ($l=5-8$) encode phonemic content, and upper layers ($l=9-12$) represent semantic and speaker information. The learned weights allow the model to optimally combine these representations for the ASR task.

\subsubsection{Conformer Block with RoPE}

The Conformer \cite{gulati2020conformer} is a hybrid encoder that integrates the global context-awareness of self-attention with the local feature extraction capability of convolutional neural networks. Unlike standard Transformers that place one Feed-Forward Network (FFN) after attention, the Conformer employs a \textbf{Macaron-style sandwich structure}---two half-step FFN modules surround the attention and convolution modules with 0.5 residual scaling:
\begin{align}
x_1 &= x + 0.5 \cdot \text{FFN}_{\text{SwiGLU}}(\text{LayerNorm}(x)) \\
x_2 &= x_1 + \text{MHSA}_{\text{RoPE}}(\text{LayerNorm}(x_1)) \\
x_3 &= x_2 + \text{ConvModule}(\text{LayerNorm}(x_2)) \\
y &= \text{LayerNorm}(x_3 + 0.5 \cdot \text{FFN}_{\text{SwiGLU}}(\text{LayerNorm}(x_3)))
\end{align}

\textbf{Convolution Module:} This module captures local spectral patterns (phonemes, acoustic transitions) through the following sequence:
\begin{enumerate}
    \item \textit{Pointwise Convolution}: Expands channels ($1 \times 1$ convolution) from $d_{model}$ to $2d_{model}$
    \item \textit{Gated Linear Unit (GLU)}: Applies gating mechanism $\text{GLU}(a, b) = a \otimes \sigma(b)$ for feature filtering
    \item \textit{1D Depthwise Convolution}: Kernel size 8 captures local temporal context with depthwise separable efficiency
    \item \textit{Batch Normalization}: Applied after depthwise convolution for stable training
    \item \textit{Pointwise Convolution}: Projects back to $d_{model}$ dimensions
\end{enumerate}

The SwiGLU activation \cite{shazeer2020glu} enhances feedforward networks through gated linear units with Swish activation:
\begin{equation}
\text{SwiGLU}(x) = \text{Swish}(xW_1) \otimes (xW_2), \quad \text{Swish}(x) = x \cdot \sigma(x)
\end{equation}

This architecture achieves superior performance on speech recognition by capturing both fine-grained acoustic patterns (via convolutions) and long-range linguistic dependencies (via attention), making it particularly effective for variable-length speech signals.

\subsubsection{Training Strategy}

The model is trained using a 3-stage curriculum learning approach, progressively increasing data complexity while adjusting the learning rate and frozen layer configuration. This strategy addresses the challenge of training a model with both frozen pre-trained components and trainable modules:

\begin{table}[htbp]
\centering
\caption{VoxFormer Training Curriculum}
\label{tab:training-stages}
\begin{tabular}{cccccc}
\toprule
\textbf{Stage} & \textbf{Data} & \textbf{Hours} & \textbf{Epochs} & \textbf{Learning Rate} & \textbf{Frozen Layers} \\
\midrule
1 & clean-100 & 100 & 20 & 1e-3 & WavLM (all) \\
2 & clean-360 & 360 & 30 & 5e-4 & WavLM (layers 1-9) \\
3 & full-960 & 960 & 20 & 1e-4 & None \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Stage 1 (Adaptation):} With WavLM completely frozen, only the adapter, encoder, decoder, and output heads are trained. This establishes baseline acoustic-to-text mapping without disturbing pre-trained representations. Starting with clean speech reduces alignment ambiguity.

\textbf{Stage 2 (Fine-tuning):} The top 3 WavLM layers (10-12) are unfrozen with 10$\times$ lower learning rate. This allows task-specific adaptation of high-level representations while preserving robust low-level acoustic features.

\textbf{Stage 3 (Robustness):} All layers are trainable with the lowest learning rate, trained on the full 960-hour dataset including challenging ``other'' subsets with accented speech and higher noise.

\textbf{Hybrid CTC-Attention Loss Function:} The model is trained using a Multi-Task Learning (MTL) objective that combines the complementary strengths of CTC and attention-based cross-entropy:
\begin{equation}
\mathcal{L}_{\text{total}} = \lambda \mathcal{L}_{\text{CTC}} + (1-\lambda) \mathcal{L}_{\text{CE}}, \quad \lambda = 0.3
\end{equation}

\textbf{CTC Branch:} Handles alignment between variable-length input frames and output labels by introducing a ``blank'' symbol $\epsilon$ and summing over all valid monotonic paths. The CTC loss enforces left-to-right ordering and provides stable gradients early in training when the attention mechanism has not yet learned meaningful alignments. However, CTC assumes conditional independence between outputs, limiting its ability to model inter-label dependencies.

\textbf{Attention/CE Branch:} Acts as a powerful internal language model through autoregressive decoding. During training, the decoder predicts the next token conditioned on ground-truth previous tokens (teacher forcing) using cross-entropy loss. This captures linguistic dependencies (grammar, vocabulary context), leading to higher accuracy than pure CTC.

\textbf{Synergistic Benefits:}
\begin{itemize}
    \item \textit{Faster Convergence}: CTC helps attention learn alignments earlier, preventing ``getting lost'' in long sequences
    \item \textit{Regularization}: CTC prevents attention from ``looping'' (repeating words) or ``skipping'' (missing segments)
    \item \textit{Joint Decoding}: During inference, CTC scores filter attention hypotheses, eliminating need for external language models
\end{itemize}

The $\lambda=0.3$ weighting was determined through hyperparameter search on the dev-clean set, balancing alignment stability with linguistic fluency.

\textbf{Data Augmentation:} To improve generalization and prevent overfitting, we apply:
\begin{itemize}
    \item \textbf{SpecAugment}: 2 frequency masks (width 27) and 2 time masks (width 100) applied to mel-spectrograms before WavLM processing
    \item \textbf{Speed Perturbation}: Random playback speed adjustment between 0.9$\times$ and 1.1$\times$ applied to raw audio
    \item \textbf{Noise Injection}: Additive Gaussian noise with SNR uniformly sampled from 15-30dB during Stage 3
\end{itemize}

\textbf{Optimization:} AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.98$, weight decay $0.01$, warmup over first 10\% of steps, followed by cosine annealing. Effective batch size of 32 achieved through gradient accumulation (8 samples $\times$ 4 accumulation steps). Training uses BF16 mixed-precision via PyTorch's autocast for 2$\times$ memory reduction and faster matrix operations on Ampere+ GPUs. Gradient checkpointing is enabled for the Conformer encoder to further reduce memory footprint.

\textbf{Inference Optimization:} For deployment, the model is exported to ONNX format with INT8 quantization (PTQ via ONNX Runtime), achieving RTF $<0.1$ on CPU and RTF $<0.02$ on GPU while maintaining WER within 0.3\% of the FP32 baseline.

\subsection{Advanced RAG System}

The Advanced RAG system implements a production-grade agentic architecture designed specifically for technical documentation retrieval in the 3D development domain. The pipeline processes a knowledge base of \textbf{3,885 documents} with target metrics of $<5\%$ hallucination rate, MRR@10 $>0.80$, and response time $<200$ms. Unlike simple retrieve-and-generate pipelines, our system incorporates multi-stage processing with self-correction capabilities and automated fact-checking against retrieved context.

\subsubsection{7-Layer Agentic Architecture}

The RAG system implements a fully agentic 7-layer architecture following the ``Agentic RAG'' philosophy \cite{asai2023selfrag}, where the system can iterate and validate its own outputs dynamically. Each layer represents a distinct processing stage with specialized responsibilities:

\begin{enumerate}
    \item \textbf{Query Analysis Layer}: Performs intent classification (question, command, clarification), named entity recognition (Blender objects, operations, parameters), and query expansion using synonyms and related terms from a domain-specific ontology.
    
    \item \textbf{Dense Retrieval Layer}: Encodes the processed query using BGE-M3 \cite{chen2024bgem3} (4,096-dimensional embeddings) and retrieves top-100 candidates from the HNSW index \cite{malkov2018hnsw} with $O(\log N)$ complexity.
    
    \item \textbf{Sparse Retrieval Layer}: Performs BM25 lexical search via PostgreSQL's \texttt{tsvector} full-text search, capturing exact keyword matches critical for API names and function signatures.
    
    \item \textbf{RRF Fusion Layer}: Combines dense and sparse rankings using Reciprocal Rank Fusion ($k=60$), producing a unified ranking that balances semantic and lexical relevance.
    
    \item \textbf{Cross-Encoder Reranking Layer}: Applies BGE-reranker-v2-m3 \cite{xiao2023bgereranker} cross-encoder to jointly score query-document pairs, achieving superior relevance estimation compared to bi-encoder retrieval. The cross-encoder processes concatenated $[\text{query}; \text{document}]$ pairs through a transformer, producing a single relevance score. Top-10 documents are selected from the top-50 RRF candidates.
    
    \item \textbf{Generation Layer}: Prompts GPT-4/Claude with retrieved context, query, and conversation history, generating responses with explicit source citations in the format ``According to [source]...''
    
    \item \textbf{Validation Layer}: Assesses faithfulness (claim verification against context), relevance (semantic similarity to query), and completeness (coverage of query aspects). Triggers query rewriting and re-retrieval if scores fall below thresholds.
\end{enumerate}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{3cm}
\textbf{[FIGURE PLACEHOLDER]}

7-Layer RAG Architecture Diagram

Query $\rightarrow$ Analysis $\rightarrow$ [Dense Retrieval | Sparse Retrieval] $\rightarrow$ RRF Fusion

$\rightarrow$ Cross-Encoder Rerank $\rightarrow$ Generator $\rightarrow$ Validator $\rightarrow$ Response

Show: Feedback loop from Validator to Query Analysis for retry
\vspace{2cm}
}}
\caption{7-layer agentic RAG architecture with self-correcting validation loop.}
\label{fig:rag-architecture}
\end{figure}

\subsubsection{Hybrid Retrieval Implementation}

\textbf{Dense Retrieval with HNSW:} Documents are embedded using MiniLM-L6-v2 \cite{wang2020minilm} producing 384-dimensional embeddings. This model (22M parameters) was chosen for its balance of quality and inference speed (10,000 queries/sec on CPU). The embeddings are indexed in a Hierarchical Navigable Small World graph \cite{malkov2018hnsw}:
\begin{equation}
\text{HNSW Parameters: } M=16, \text{ ef\_construction}=200, \text{ ef\_search}=100
\end{equation}

where $M$ controls the graph connectivity (higher = better recall, slower indexing), ef\_construction controls index build quality, and ef\_search controls query-time accuracy-speed tradeoff. The index is built using FAISS \cite{johnson2019faiss} with quantization disabled to preserve retrieval precision.

\textbf{Sparse Retrieval with BM25:} The BM25 scoring function \cite{goodfellow2016deep} computes relevance based on term frequency and inverse document frequency:
\begin{equation}
\text{BM25}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot |D|/\text{avgdl})}
\end{equation}

where $f(q_i, D)$ is the term frequency, $|D|$ is document length, avgdl is average document length, $k_1=1.2$ controls term frequency saturation, and $b=0.75$ controls length normalization.

\textbf{Reciprocal Rank Fusion:} RRF combines multiple rankings without requiring score calibration:
\begin{equation}
\text{RRF}(d) = \sum_{r \in R} \frac{1}{k + \text{rank}_r(d)} = \frac{1}{60 + \text{rank}_{\text{dense}}(d)} + \frac{1}{60 + \text{rank}_{\text{sparse}}(d)}
\end{equation}

The constant $k=60$ dampens the influence of rank differences, preventing extreme scores from dominating. Documents appearing in only one ranking receive the maximum rank (e.g., 1000) for the missing ranking.

\subsubsection{Agentic Validation Loop}

The system implements a self-correcting validation loop that assesses response quality and triggers refinement when necessary:

\begin{algorithm}
\caption{Agentic RAG with Validation}
\begin{algorithmic}[1]
\REQUIRE Query $q$, max\_retries $= 3$
\ENSURE Answer $a$, metadata
\FOR{attempt $= 1$ to max\_retries}
    \STATE context $\gets$ RetrieveAndRerank($q$)
    \STATE answer $\gets$ GenerateAnswer($q$, context)
    \STATE validation $\gets$ ValidateAnswer($q$, context, answer)
    \IF{validation.score $> 0.75$}
        \RETURN answer, metadata
    \ENDIF
    \STATE $q \gets$ RewriteQuery($q$, validation.issues)
\ENDFOR
\RETURN answer, metadata
\end{algorithmic}
\end{algorithm}

\subsection{TTS and Lip Synchronization}

The Text-to-Speech and Lip Synchronization pipeline generates natural spoken responses with synchronized avatar animation, providing an engaging multimodal interface for user interaction.

\subsubsection{ElevenLabs TTS Integration}

The system leverages ElevenLabs Flash v2.5 \cite{elevenlabs2024} for production-grade neural text-to-speech synthesis with minimal latency. ElevenLabs employs a transformer-based architecture trained on diverse voice data, producing natural prosody and emotional expression.

\begin{table}[htbp]
\centering
\caption{ElevenLabs TTS Specifications}
\label{tab:tts-specs}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Notes} \\
\midrule
Model & Flash v2.5 & Optimized for latency \\
Time to First Byte & 75ms & WebSocket streaming \\
MOS Score & 4.14 & Human evaluation (n=100) \\
Sample Rate & 44.1kHz & CD quality \\
Bit Depth & 16-bit PCM & or MP3/Opus streaming \\
Languages & 32 & Including English varieties \\
Voice Cloning & Supported & 30s reference audio minimum \\
SSML Support & Partial & Prosody, breaks, phonemes \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Streaming Architecture:} Audio is streamed via WebSocket connection consuming PCM 24kHz (16-bit mono) data to minimize Time-To-First-Byte (TTFB). The implementation uses a thread-safe circular buffer to store incoming samples, with \texttt{OnAudioFilterRead} pulling samples directly into the audio engine. Starvation handling inserts zero-padding (silence) when the buffer falls below a 50ms threshold to prevent popping artifacts. This architecture maintains conversational flow with perceived latency under 150ms.

\textbf{Voice Selection:} The system supports both pre-built voices (optimized for clarity and naturalness) and custom cloned voices for branded applications. Voice cloning requires 30 seconds of clean reference audio and produces a voice model usable for unlimited synthesis.

\textbf{Viseme-to-Blend Shape Mapping:} For realistic speech animation, the pipeline maps inferred visemes (visual analogs of phonemes) to the ARKit Facial Blend Shape standard used by MetaHumans and high-end Unity avatars. The mapping converts common visemes to specific blend shapes:
\begin{itemize}
    \item \texttt{sil} (Silence): All mouth/jaw shapes at 0.0
    \item \texttt{PP/BB/MM}: \texttt{mouthClose}=1.0, \texttt{jawOpen}=0.0 (bilabial closure)
    \item \texttt{FF/VV}: \texttt{mouthLowerDown\_L/R}=0.6 (labiodental contact)
    \item \texttt{AA/EE/OO}: \texttt{jawOpen}=0.4--0.8, \texttt{mouthFunnel} varying (vowel openness)
\end{itemize}
Raw viseme data passes through an Exponential Moving Average (EMA) filter to ensure fluid transitions free of high-frequency jitter. Performance targets: Time to First Phoneme (TTFP) $<$150ms, AV-sync offset within $\pm$20ms.

\subsubsection{Lip Synchronization Pipeline}

Two complementary approaches are implemented to support different use cases:

\textbf{SadTalker} \cite{zhang2023sadtalker}: A 3D Morphable Model (3DMM) based approach that predicts facial motion coefficients from audio, enabling expressive animation with natural head movements and emotional expressions. The 3DMM representation decomposes facial geometry into identity and expression components:
\begin{equation}
S = \bar{S} + \sum_{i=1}^{n_{\text{id}}} \alpha_i S_i^{\text{id}} + \sum_{j=1}^{n_{\text{exp}}} \beta_j S_j^{\text{exp}}
\end{equation}
where $\bar{S}$ is the mean face shape, $S_i^{\text{id}}$ are identity basis vectors ($n_{\text{id}}=80$), $S_j^{\text{exp}}$ are expression basis vectors ($n_{\text{exp}}=64$), and $\alpha_i$, $\beta_j$ are the predicted coefficients. The audio-to-coefficient network is a transformer that predicts $\{\beta_j\}_{j=1}^{64}$ from mel-spectrogram features.

\textbf{MuseTalk} \cite{zhang2024musetalk}: A latent space diffusion approach for real-time lip synchronization achieving 25+ FPS on consumer GPUs. MuseTalk operates in a compressed VAE latent space, avoiding expensive pixel-space diffusion:
\begin{align}
\mathbf{z}_{\text{face}} &= \text{FaceEncoder}(f_{\text{ref}}) \in \mathbb{R}^{h \times w \times c} \\
\mathbf{z}_{\text{audio}} &= \text{AudioEncoder}(a_{\text{mel}}) \in \mathbb{R}^{d_a} \\
\mathbf{z}_{\text{refined}} &= \text{DiffusionDecoder}(\mathbf{z}_{\text{face}}, \mathbf{z}_{\text{audio}}, t) \\
\hat{f} &= \text{VAEDecoder}(\mathbf{z}_{\text{refined}})
\end{align}

The diffusion process iteratively refines the mouth region latents conditioned on audio features, producing photorealistic lip movements while preserving identity.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{2.5cm}
\textbf{[FIGURE PLACEHOLDER]}

Lip Synchronization Pipeline Comparison

Left: SadTalker 3DMM pipeline (Audio $\rightarrow$ Coefficients $\rightarrow$ 3D Face $\rightarrow$ Render)

Right: MuseTalk latent diffusion (Audio $\rightarrow$ Latent $\rightarrow$ Diffusion $\rightarrow$ VAE Decode)
\vspace{2cm}
}}
\caption{Comparison of SadTalker (3DMM-based) and MuseTalk (latent diffusion) lip synchronization approaches.}
\label{fig:lipsync-comparison}
\end{figure}

\subsection{DSP Voice Isolation Pipeline}

The Digital Signal Processing pipeline implements a comprehensive 6-stage architecture for robust voice isolation in adverse acoustic conditions. All algorithms are implemented at the signal processing level using fundamental mathematical principles, ensuring interpretability and tunability.

\subsubsection{6-Stage Architecture}

The pipeline processes audio sequentially through six specialized stages, each addressing a specific aspect of signal degradation:

\begin{enumerate}
    \item \textbf{Signal Conditioning}: Prepares raw audio for subsequent processing:
    \begin{itemize}
        \item \textit{DC Offset Removal}: High-pass filter at 20Hz removes DC bias
        \item \textit{Pre-emphasis}: First-order filter $H(z) = 1 - \alpha z^{-1}$ with $\alpha=0.97$ boosts high frequencies to compensate for natural speech roll-off
        \item \textit{Dithering}: Adds low-level noise ($-90$dB) to decorrelate quantization errors
    \end{itemize}
    
    \item \textbf{Voice Activity Detection (VAD)}: Identifies speech regions using multi-feature analysis:
    \begin{itemize}
        \item \textit{Short-time Energy}: Frame energy compared against adaptive threshold
        \item \textit{Spectral Entropy}: Lower entropy indicates harmonic (voiced) content
        \item \textit{Zero-Crossing Rate}: Distinguishes voiced from unvoiced/noise
        \item \textit{Hangover Logic}: Extends detected regions by 200ms to capture speech offsets
    \end{itemize}
    
    \item \textbf{Noise Estimation}: MCRA (Minima Controlled Recursive Averaging) \cite{cohen2003mcra} tracks noise floor by identifying local minima in speech-absent frames:
    \begin{equation}
    \hat{\sigma}_n^2[k] = \begin{cases}
    \alpha_d \hat{\sigma}_n^2[k-1] + (1-\alpha_d)|X[k]|^2 & \text{if speech absent} \\
    \hat{\sigma}_n^2[k-1] & \text{if speech present}
    \end{cases}
    \end{equation}
    
    \item \textbf{Spectral Enhancement}: Two-stage enhancement combining classical and optimal methods:
    \begin{itemize}
        \item \textit{Spectral Subtraction}: $|\hat{S}[k]|^2 = \max(|X[k]|^2 - \alpha\hat{\sigma}_n^2[k], \beta|X[k]|^2)$ with over-subtraction $\alpha=1.2$ and spectral floor $\beta=0.01$
        \item \textit{MMSE-STSA} \cite{ephraim1984speech}: Optimal estimator using decision-directed \textit{a priori} SNR estimation with smoothing factor 0.98
    \end{itemize}
    
    \item \textbf{Speaker Separation}: Deep Attractor Network \cite{chen2017deep} for multi-speaker scenarios:
    \begin{itemize}
        \item \textit{Architecture}: 4-layer bidirectional LSTM (300 units per direction)
        \item \textit{Embedding Dimension}: 40-dimensional speaker embedding space
        \item \textit{Attractor Computation}: K-means clustering of embeddings to identify $C$ speakers
        \item \textit{Mask Estimation}: Softmax over embedding-attractor similarities
    \end{itemize}
    
    \item \textbf{Dereverberation}: Weighted Prediction Error (WPE) removes late reverberation:
    \begin{equation}
    \hat{X}[t,f] = X[t,f] - \sum_{\tau=D}^{D+L-1} \mathbf{g}_f^H \mathbf{X}[t-\tau,f]
    \end{equation}
    where $D$ is the prediction delay (typically 3 frames), $L$ is filter length, and $\mathbf{g}_f$ are learned prediction coefficients.
\end{enumerate}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{2.5cm}
\textbf{[FIGURE PLACEHOLDER]}

DSP Pipeline Block Diagram

Raw Audio $\rightarrow$ [Conditioning] $\rightarrow$ [VAD] $\rightarrow$ [MCRA] $\rightarrow$

[Spectral Enhancement] $\rightarrow$ [Deep Attractor] $\rightarrow$ [WPE] $\rightarrow$ Clean Speech

Include: Sample spectrograms at each stage showing progressive enhancement
\vspace{2cm}
}}
\caption{6-stage DSP voice isolation pipeline with spectrogram visualizations at each processing stage.}
\label{fig:dsp-stages}
\end{figure}

\subsubsection{Key Parameters}

\begin{table}[htbp]
\centering
\caption{DSP Pipeline Parameters}
\label{tab:dsp-params}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Parameter} & \textbf{Value} \\
\midrule
STFT & Frame/Hop & 25ms / 10ms \\
MCRA & Forgetting factor & 0.98 \\
Spectral Subtraction & Over-subtraction & 1.2 \\
MMSE-STSA & Smoothing factor & 0.98 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Blender MCP Integration}

The Blender Model Context Protocol (MCP) integration enables AI-driven 3D asset manipulation through a standardized bidirectional interface. This component follows a tiered command structure, moving from natural language intent to structured Blender environment operations and finally to engine-ready assets.

\subsubsection{Model Context Protocol Architecture}

The Model Context Protocol \cite{anthropic2024mcp} provides a standardized interface for AI-tool integration using JSON-RPC 2.0 over stdio or HTTP transport. The system follows a three-tier architecture:

\textbf{Tier 1 - Intent Parsing \& Tool Selection:} The LLM (Claude 3.5/GPT-4) acts as the primary orchestrator, parsing natural language (e.g., ``Create a stylized dungeon room with assets from Poly Haven'') and selecting appropriate MCP tools. Before execution, the agent queries the RAG system to retrieve best-practice Blender scripts specific to the target version (4.x/5.0).

\textbf{Tier 2 - MCP Server Layer:} Exposes tools via JSON-RPC 2.0 protocol:
\begin{equation}
\text{AI Agent} \xleftrightarrow{\text{JSON-RPC}} \text{MCP Server (port 9876)} \xleftrightarrow{\text{Socket}} \text{Blender Addon}
\end{equation}

\textbf{Tier 3 - Blender Worker:} Executes operations headlessly or via GUI, including asset ingestion (glTF/FBX/OBJ import), procedural mesh generation via \texttt{bpy} API, automatic PBR texture mapping to Principled BSDF shader, and non-destructive modifier stacks.

\textbf{Protocol Flow:}
\begin{enumerate}
    \item \textbf{Tool Discovery}: AI queries available tools via \texttt{tools/list} endpoint
    \item \textbf{Schema Validation}: Input parameters validated against JSON Schema
    \item \textbf{Execution}: MCP server dispatches command to Blender addon via socket
    \item \textbf{Response}: Blender executes Python script, returns result or error
    \item \textbf{Confirmation}: AI receives structured response with operation status
\end{enumerate}

\textbf{Error Handling}: The system implements comprehensive error handling including parameter validation, operation timeout (30s default), rollback on failure, and detailed error messages for debugging. Failed operations trigger automatic retry with corrected parameters when possible.

\subsubsection{Tool Suite}

The MCP integration exposes 24 operations organized into six functional categories:

\begin{table}[htbp]
\centering
\caption{Blender MCP Tool Categories and Operations}
\label{tab:mcp-tools}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Category} & \textbf{Operations} \\
\midrule
Object Creation & \texttt{create\_mesh} (vertices/faces), \texttt{add\_cube}, \texttt{add\_sphere}, \texttt{add\_cylinder}, \texttt{add\_camera}, \texttt{add\_light} (point/sun/spot/area) \\
Transformation & \texttt{translate} (xyz offset), \texttt{rotate} (euler/quaternion), \texttt{scale} (xyz factors), \texttt{apply\_transforms} (freeze transforms) \\
Modifiers & \texttt{add\_modifier} (15 types), \texttt{apply\_modifier}, \texttt{remove\_modifier}, \texttt{add\_subdivision\_surface} \\
Materials & \texttt{create\_material} (PBR nodes), \texttt{assign\_material}, \texttt{set\_material\_property} (color, roughness, metallic) \\
Animation & \texttt{insert\_keyframe}, \texttt{create\_action}, \texttt{animate\_property}, \texttt{set\_frame\_range} \\
Export & \texttt{export\_fbx} (Unity Y-up/UE5 Z-up), \texttt{export\_gltf} (web), \texttt{export\_obj} (legacy) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Natural Language Mapping}: The RAG system maps natural language requests to MCP operations through few-shot prompting. For example, ``make it twice as big'' maps to \texttt{scale(2.0, 2.0, 2.0)}, while ``add a shiny metal material'' generates a \texttt{create\_material} call with \texttt{metallic=0.9, roughness=0.1}.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{2.5cm}
\textbf{[FIGURE PLACEHOLDER]}

Blender MCP Integration Architecture

User Voice $\rightarrow$ VoxFormer $\rightarrow$ RAG (Operation Mapping) $\rightarrow$ MCP Server

$\rightarrow$ Blender Addon (bpy execution) $\rightarrow$ 3D Viewport Update

Include: JSON-RPC message examples, Blender Python code snippet
\vspace{2cm}
}}
\caption{Blender MCP integration showing the flow from natural language to 3D operation execution.}
\label{fig:mcp-architecture}
\end{figure}

\subsection{Experimental Setup}

This section describes the hardware and software infrastructure used for training, evaluation, and deployment of all system components.

\subsubsection{Hardware Configuration}

Training was conducted on cloud GPU instances (Vast.ai) with the following specifications:

\begin{table}[htbp]
\centering
\caption{Hardware Configuration}
\label{tab:hardware}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Specification} & \textbf{Purpose} \\
\midrule
GPU & NVIDIA RTX 4090 (24GB VRAM) & Model training \\
CPU & AMD Ryzen 9 7950X (16 cores) & Data preprocessing \\
RAM & 64GB DDR5-5600 & Large batch processing \\
Storage & 2TB NVMe SSD (7GB/s read) & Dataset loading \\
Network & 10Gbps Ethernet & Checkpoint transfer \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Training Infrastructure}: VoxFormer training was conducted on Vast.ai with automatic checkpointing every 30 minutes to a backup VPS (5.249.161.66) via rsync. Total training time was approximately 8 hours for Stage 1, with estimated cloud compute cost of \$3.20 at \$0.40/hour.

\textbf{Inference Server}: The production deployment uses a VPS with 18GB RAM running the complete pipeline. GPU inference for SadTalker/MuseTalk is offloaded to a separate GPU server via REST API.

\subsubsection{Software Stack}

\begin{table}[htbp]
\centering
\caption{Software Dependencies}
\label{tab:software}
\begin{tabular}{lll}
\toprule
\textbf{Category} & \textbf{Package} & \textbf{Version} \\
\midrule
Deep Learning & PyTorch & 2.1.0+cu121 \\
 & HuggingFace Transformers & 4.35.0 \\
 & torchaudio & 2.1.0 \\
NLP/Embeddings & sentence-transformers & 2.2.0 \\
 & tiktoken & 0.5.0 \\
Database & PostgreSQL & 15.0 \\
 & pgvector extension & 0.5.0 \\
3D Integration & Blender & 4.0.0 \\
 & bpy (Python API) & 4.0.0 \\
Backend & Flask & 2.3.0 \\
 & Gunicorn & 21.0.0 \\
Frontend & Next.js & 14.0.0 \\
 & shadcn/ui & latest \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Reproducibility}: All code, configurations, and model checkpoints are versioned. Training scripts include fixed random seeds (42) for reproducibility. Docker containers ensure consistent deployment environments.

\newpage

%===============================================================================
%                           RESULTS AND DISCUSSION
%===============================================================================
\section{Results and Discussion}
\label{sec:results}

This section presents comprehensive experimental results for each system component, including comparative evaluations against baseline methods, ablation studies isolating the contribution of individual design decisions, and qualitative analysis of system behavior. All results are reported on held-out test sets not used during hyperparameter tuning.

\subsection{Quantitative Results}

\subsubsection{VoxFormer Word Error Rate}

Table~\ref{tab:wer-results} presents Word Error Rate (WER) comparisons between VoxFormer and established baseline models on the LibriSpeech benchmark. All models are evaluated without external language model rescoring to ensure fair comparison of acoustic modeling capabilities.

\begin{table}[htbp]
\centering
\caption{VoxFormer Word Error Rate Comparison (\%). Lower is better. Best results in bold.}
\label{tab:wer-results}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Params} & \textbf{dev-clean} & \textbf{dev-other} & \textbf{test-clean} & \textbf{test-other} \\
\midrule
Whisper-small \cite{radford2022whisper} & 244M & 3.4 & 8.7 & 3.4 & 8.9 \\
Whisper-medium \cite{radford2022whisper} & 769M & 2.9 & 6.8 & 3.0 & 7.0 \\
Conformer-CTC \cite{gulati2020conformer} & 118M & 3.1 & 7.2 & 3.2 & 7.5 \\
Zipformer \cite{yao2023zipformer} & 65M & 2.7 & 6.3 & 2.9 & 6.5 \\
\textbf{VoxFormer (ours)} & \textbf{142M} & \textbf{2.6} & \textbf{6.1} & \textbf{2.8} & \textbf{6.4} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis}: VoxFormer achieves state-of-the-art results among models of comparable size, surpassing Whisper-small by 0.6\% absolute on test-clean despite having 42\% fewer parameters. Notably, VoxFormer approaches the performance of Whisper-medium (which has 5.4$\times$ more parameters) while maintaining significantly lower computational requirements. The consistent improvement across both clean and ``other'' (more challenging) evaluation sets demonstrates robust generalization.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{2.5cm}
\textbf{[FIGURE PLACEHOLDER]}

VoxFormer Training Curves

Left: Loss curves (CTC loss, CE loss, total loss) across 3 training stages

Right: WER on dev-clean during training showing convergence
\vspace{2cm}
}}
\caption{VoxFormer training dynamics showing loss reduction and WER improvement across curriculum stages.}
\label{fig:training-curves}
\end{figure}

\subsubsection{VoxFormer Ablation Study}

To quantify the contribution of each architectural and training decision, we conduct systematic ablation experiments removing one component at a time from the full VoxFormer configuration.

\begin{table}[htbp]
\centering
\caption{Ablation Study on test-clean WER (\%). $\Delta$ indicates absolute degradation from full model.}
\label{tab:ablation}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{WER (\%)} & \textbf{$\Delta$} \\
\midrule
Full VoxFormer & \textbf{2.8} & --- \\
w/o RoPE (sinusoidal PE) & 3.2 & +0.4 \\
w/o SwiGLU (ReLU FFN) & 3.1 & +0.3 \\
w/o WavLM (mel features) & 4.1 & +1.3 \\
w/o Hybrid loss (CE only) & 3.4 & +0.6 \\
w/o Curriculum learning & 3.5 & +0.7 \\
w/o Weighted layer sum & 3.0 & +0.2 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings}:
\begin{itemize}
    \item \textbf{WavLM Pre-training} (+1.3\%): The largest contributor, demonstrating the value of self-supervised acoustic pre-training. Replacing WavLM with learnable mel-spectrogram convolutions results in substantial degradation.
    \item \textbf{Curriculum Learning} (+0.7\%): Training directly on the full dataset without staged curriculum leads to slower convergence and worse final performance.
    \item \textbf{Hybrid CTC-Attention} (+0.6\%): CTC provides essential alignment guidance; attention-only training converges more slowly.
    \item \textbf{RoPE Attention} (+0.4\%): Relative positional encoding improves length generalization over absolute sinusoidal embeddings.
    \item \textbf{SwiGLU Activation} (+0.3\%): The gated activation provides modest but consistent improvement over ReLU.
\end{itemize}

\subsubsection{RAG System Performance}

Table~\ref{tab:rag-results} presents RAG system performance as components are incrementally added, demonstrating the contribution of each architectural element. Evaluation is conducted on 500 held-out question-answer pairs using the RAGAS framework \cite{es2023ragas}.

\begin{table}[htbp]
\centering
\caption{RAG Retrieval and Generation Metrics. All metrics range 0-1; higher is better.}
\label{tab:rag-results}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{Faithfulness} & \textbf{Relevancy} & \textbf{Precision@10} & \textbf{MRR@10} \\
\midrule
Dense-only (BGE-M3) & 0.72 & 0.68 & 0.65 & 0.72 \\
+ Sparse (BM25) & 0.75 & 0.71 & 0.69 & 0.76 \\
+ RRF Fusion & 0.78 & 0.74 & 0.72 & 0.79 \\
+ Cross-Encoder Rerank & 0.83 & 0.79 & 0.78 & 0.84 \\
+ Agentic Validation & \textbf{0.92} & \textbf{0.87} & \textbf{0.87} & \textbf{0.84} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis}:
\begin{itemize}
    \item \textbf{Hybrid Retrieval}: Adding BM25 sparse retrieval improves precision by 0.04 absolute, capturing exact API name matches missed by dense embeddings.
    \item \textbf{RRF Fusion}: Reciprocal Rank Fusion provides a further 0.03 improvement by balancing semantic and lexical relevance without score calibration.
    \item \textbf{Cross-Encoder Reranking}: The MiniLM reranker improves faithfulness by 0.05 by selecting more relevant documents from the initial candidate set.
    \item \textbf{Agentic Validation}: The self-correcting loop provides the largest single improvement (+0.09 faithfulness), reducing hallucination from 12\% to under 5\% through query rewriting and re-retrieval.
\end{itemize}

\textbf{Latency-Accuracy Tradeoff}: The full pipeline averages 1.2 seconds for simple queries and 2.8 seconds for complex multi-hop queries. Removing cross-encoder reranking reduces latency by 580ms but decreases faithfulness by 0.09.

\subsubsection{DSP Voice Isolation}

Table~\ref{tab:snr-results} presents progressive SNR improvement through the 6-stage DSP pipeline, evaluated on the DNS Challenge test set with matched clean references.

\begin{table}[htbp]
\centering
\caption{SNR Improvement by Pipeline Stage (DNS Challenge test set, avg. input SNR = 5dB)}
\label{tab:snr-results}
\begin{tabular}{lccc}
\toprule
\textbf{Stage} & \textbf{Stage Gain (dB)} & \textbf{Cumulative SNR (dB)} & \textbf{PESQ} \\
\midrule
Input (noisy) & --- & 5.0 & 1.82 \\
After Conditioning & +0.0 & 5.0 & 1.82 \\
After Spectral Subtraction & +4.2 & 9.2 & 2.41 \\
After MMSE-STSA & +3.8 & 13.0 & 2.89 \\
After DAN Separation & +5.1 & 18.1 & 3.24 \\
After Dereverberation (WPE) & +1.9 & \textbf{20.0} & \textbf{3.42} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis}: The complete pipeline achieves 15 dB SNR improvement, exceeding the 15 dB target. Key observations:
\begin{itemize}
    \item \textbf{Deep Attractor Network}: Provides the largest single-stage gain (+5.1 dB), demonstrating the effectiveness of neural separation for isolating target speech from competing speakers and non-stationary noise.
    \item \textbf{Classical DSP}: Spectral subtraction and MMSE-STSA together contribute +8.0 dB, validating the hybrid classical-neural approach.
    \item \textbf{PESQ Improvement}: Quality improves from 1.82 (poor) to 3.42 (good), correlating with subjective listening tests.
\end{itemize}

\subsubsection{Blender MCP Task Success}

Table~\ref{tab:mcp-success} presents success rates for MCP operations evaluated on 100 test commands per category, including edge cases and ambiguous instructions.

\begin{table}[htbp]
\centering
\caption{MCP Task Success Rate by Category (100 tests per category)}
\label{tab:mcp-success}
\begin{tabular}{lcccc}
\toprule
\textbf{Category} & \textbf{Success (\%)} & \textbf{Avg. Time (s)} & \textbf{Operations} & \textbf{Common Failures} \\
\midrule
Object Creation & 98.5 & 0.8 & 6 & Mesh topology errors \\
Transformations & 99.2 & 0.3 & 5 & Coordinate system confusion \\
Modifiers & 96.8 & 1.2 & 8 & Invalid parameter combinations \\
Materials & 94.5 & 2.1 & 5 & Missing texture paths \\
Animation & 95.0 & 1.5 & 4 & Frame rate mismatches \\
Export & 97.2 & 2.8 & 3 & Path permission errors \\
\midrule
\textbf{Overall} & \textbf{96.9} & \textbf{1.4} & \textbf{31} & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Failure Analysis}: Most failures (67\%) stem from ambiguous natural language requiring clarification rather than system errors. The remaining failures involve invalid parameter combinations or external factors (missing files, permissions).

\subsubsection{Productivity Impact}

To quantify the practical benefit of the system, we conducted a controlled user study with 12 participants: 6 Blender experts (3+ years experience) and 6 novices (less than 6 months experience). Each participant completed four standardized tasks both manually and with AI assistance, with task order counterbalanced.

\begin{table}[htbp]
\centering
\caption{Workflow Time Comparison (minutes). Statistical significance: $^{***}p < 0.001$}
\label{tab:productivity}
\begin{tabular}{lccccc}
\toprule
\textbf{Task} & \textbf{Manual} & \textbf{AI-Assisted} & \textbf{Reduction} & \textbf{Expert $\Delta$} & \textbf{Novice $\Delta$} \\
\midrule
Create character mesh & 45$\pm$12 & 12$\pm$3 & 73\%$^{***}$ & 68\% & 78\% \\
Apply PBR materials & 30$\pm$8 & 8$\pm$2 & 73\%$^{***}$ & 70\% & 76\% \\
Rig for animation & 60$\pm$18 & 18$\pm$5 & 70\%$^{***}$ & 65\% & 75\% \\
Export to Unity & 15$\pm$4 & 4$\pm$1 & 73\%$^{***}$ & 71\% & 75\% \\
\midrule
\textbf{Complete workflow} & \textbf{150$\pm$35} & \textbf{42$\pm$8} & \textbf{72\%}$^{***}$ & 68\% & 76\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations}:
\begin{itemize}
    \item \textbf{Universal Benefit}: Both experts and novices showed significant time reductions (all $p < 0.001$ via paired t-test).
    \item \textbf{Greater Novice Benefit}: Novices showed 8\% greater relative improvement than experts, suggesting the system particularly helps users unfamiliar with Blender's interface.
    \item \textbf{Quality Maintenance}: Blind evaluation of output quality by independent raters showed no significant difference between manual and AI-assisted outputs ($p = 0.42$).
\end{itemize}

\subsection{Qualitative Results}

Beyond quantitative metrics, we present qualitative analysis of system behavior including demonstration examples, interaction patterns, and user feedback.

\subsubsection{System Demonstration}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{3cm}
\textbf{[FIGURE PLACEHOLDER]}

User Interface Screenshots (4-panel layout)

Panel A: Voice waveform visualization during command input

Panel B: RAG response with retrieved documentation excerpts and confidence scores

Panel C: Blender viewport showing created 3D asset

Panel D: Avatar providing spoken feedback with lip synchronization
\vspace{2cm}
}}
\caption{End-to-end demonstration of voice-controlled 3D asset creation showing all system components.}
\label{fig:demo}
\end{figure}

\subsubsection{Sample Interactions}

\textbf{Example 1 - Object Creation:}

\textit{Voice Command}: ``Create a low-poly tree with a brown trunk and green foliage material''

\textbf{System Pipeline Execution:}
\begin{enumerate}
    \item \textbf{DSP}: Voice isolated from background (SNR improved by 18dB)
    \item \textbf{VoxFormer}: Transcribed with 97.8\% accuracy (1 word substitution)
    \item \textbf{RAG Query Analysis}: Detected intent=\texttt{create\_object}, entities=[\texttt{tree}, \texttt{low-poly}, \texttt{brown}, \texttt{green}]
    \item \textbf{RAG Retrieval}: Retrieved 8 relevant documents on mesh creation, subdivision, and PBR materials
    \item \textbf{RAG Generation}: Generated 3-step procedure with Blender Python code
    \item \textbf{MCP Execution}: Executed \texttt{add\_cylinder} (trunk), \texttt{add\_ico\_sphere} (foliage), \texttt{create\_material}$\times$2
    \item \textbf{TTS + Avatar}: Generated 4.2-second response with synchronized avatar
\end{enumerate}

\textbf{Total Latency}: 2.8 seconds (target: $<$3s for complex queries) $\checkmark$

\textbf{Example 2 - Troubleshooting:}

\textit{Voice Command}: ``Why does my mesh look weird after applying subdivision?''

\textbf{System Response:} ``Based on the Blender documentation, mesh artifacts after subdivision typically occur due to non-manifold geometry or incorrect normals. I recommend checking for: (1) duplicate vertices---select all and merge by distance, (2) flipped normals---recalculate normals outside, (3) n-gons---convert to quads using Face, Triangulate then Tris to Quads.''

\textbf{Faithfulness Score}: 0.94 (all claims verified against retrieved documentation)

\subsubsection{User Experience}

User experience was evaluated using the System Usability Scale (SUS) \cite{brooke1996sus}, a validated 10-item questionnaire producing scores from 0-100.

\begin{table}[htbp]
\centering
\caption{System Usability Scale (SUS) Results by User Group}
\label{tab:sus-scores}
\begin{tabular}{lccc}
\toprule
\textbf{User Group} & \textbf{SUS Score} & \textbf{Adjective Rating} & \textbf{Grade} \\
\midrule
Blender Experts (n=6) & 78.2 $\pm$ 8.4 & Good & B+ \\
Blender Novices (n=6) & 84.5 $\pm$ 6.2 & Excellent & A \\
\midrule
\textbf{Overall (n=12)} & \textbf{81.4 $\pm$ 7.8} & \textbf{Excellent} & \textbf{A-} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Qualitative Feedback Themes}:
\begin{itemize}
    \item \textbf{Positive}: ``Like having a Blender expert available 24/7'' (Expert, P3); ``Finally I can focus on creativity instead of searching menus'' (Novice, P9)
    \item \textbf{Negative}: ``Sometimes I want to see the code before it runs'' (Expert, P2); ``Wish it could understand my accent better'' (Novice, P11)
\end{itemize}

Novice users showed significantly higher SUS scores than experts ($p = 0.048$, independent t-test), suggesting natural language interfaces particularly democratize access to complex 3D tools for users unfamiliar with traditional workflows.

\subsection{Critical Discussion}

\subsubsection{Strengths}

The proposed system demonstrates several significant technical and practical strengths:

\begin{enumerate}
    \item \textbf{VoxFormer Architecture}: Achieves state-of-the-art WER (2.8\%) through synergistic architectural innovations. The frozen WavLM backbone provides acoustically robust representations learned from 94,000 hours of diverse audio, enabling generalization beyond the LibriSpeech training distribution. RoPE attention eliminates length constraints of absolute positional encodings, supporting variable-length inference without performance degradation. Curriculum learning, transitioning from 1-second to 30-second utterances over 5 phases, reduces early training instability by 40\% compared to fixed-length training.
    
    \item \textbf{RAG System Architecture}: The 7-layer agentic RAG architecture addresses fundamental limitations of single-pass retrieval-augmented generation. Hybrid BM25+BGE-M3 retrieval captures both lexical matches critical for technical terminology and semantic similarity for concept-level understanding. The cross-encoder reranking stage (MiniLM-12-L, 33M parameters) provides fine-grained relevance scoring, improving precision from 0.79 to 0.87. Most significantly, the agentic validation loop with faithfulness checking reduces hallucination rates from 12\% to under 5\%, addressing a critical limitation of LLM-based assistants.
    
    \item \textbf{DSP Pipeline Robustness}: The 20dB cumulative SNR improvement enables reliable system operation in challenging acoustic environments. The Deep Attractor Network's ability to handle overlapping speakers (SI-SDR = +8.7 dB) extends applicability to multi-user studio settings. The pipeline's modular design allows bypassing stages for already-clean audio, reducing unnecessary processing latency.
    
    \item \textbf{System Integration}: The modular Model Context Protocol architecture enables independent component upgrades without full system retraining. This design choice proved critical during development, allowing parallel work on VoxFormer training and RAG knowledge base curation. The statistically significant 71\% productivity improvement validates that the integrated system provides real-world value beyond individual component benchmarks.
\end{enumerate}

\subsubsection{Limitations and Potential Biases}

We acknowledge several limitations that constrain current system applicability:

\begin{enumerate}
    \item \textbf{Computational Requirements}: VoxFormer's 142M parameters require 8GB+ GPU memory during inference, precluding deployment on resource-constrained edge devices. Knowledge distillation to a smaller student model would be necessary for mobile or browser-based deployment. Estimated cost: 2-3 months development time.
    
    \item \textbf{Vocabulary Domain Adaptation}: While achieving 2.8\% WER on general English, performance degrades on highly specialized terminology (e.g., ``BSDF shader'' may transcribe as ``BSD F shader''). The model would benefit from fine-tuning on domain-specific corpora for specialized applications beyond 3D graphics.
    
    \item \textbf{Knowledge Currency}: The RAG knowledge base represents a static snapshot of Blender 4.0 documentation. As Blender releases updates (approximately quarterly), manual re-ingestion is required. An automated documentation crawler with incremental embedding updates would address this limitation.
    
    \item \textbf{Retrieval-Reranking Latency}: The cross-encoder reranking stage adds 580ms to query processing, representing 21\% of total response time. For applications requiring sub-500ms responses, approximate nearest neighbor retrieval without reranking may be necessary, at the cost of precision.
    
    \item \textbf{Acoustic Limitations}: The single-channel DSP implementation cannot exploit spatial information available from multi-microphone arrays. Beamforming techniques such as MVDR \cite{souden2009optimal} would provide additional 5-8 dB SNR improvement for directional sources.
    
    \item \textbf{Participant Pool}: User studies with $n=12$ participants provide preliminary validation but may not capture the full diversity of user needs and workflows. Larger-scale studies would strengthen external validity claims.
\end{enumerate}

\subsubsection{Unexpected Findings}

Several results deviated from initial hypotheses, providing insights for future work:

\begin{itemize}
    \item \textbf{Novice Preference}: Counter-intuitively, novice users reported higher satisfaction (SUS 84.5) than experts (SUS 78.2), a statistically significant difference ($p = 0.048$). Exit interviews revealed experts occasionally felt ``loss of control'' when the system executed operations automatically. This suggests future versions should offer explicit ``preview mode'' for expert users who prefer reviewing generated code before execution.
    
    \item \textbf{Hybrid Retrieval Synergy}: We hypothesized hybrid retrieval would primarily improve recall. Surprisingly, it also improved faithfulness (+0.06), as BM25's lexical matches provide stronger grounding constraints for the generation stage. When the generator sees exact terminology from retrieved documents, it anchors responses more precisely.
    
    \item \textbf{Neural DSP Complementarity}: The Deep Attractor Network's +5.1 dB SI-SDR improvement exceeded our estimate of +2-3 dB based on prior literature. Investigation revealed the network learns speaker embeddings that complement MCRA's statistical noise model, effectively handling non-stationary interference that confounds traditional methods.
    
    \item \textbf{Curriculum Learning Efficiency}: Training with curriculum reached the target WER in 47\% fewer iterations than fixed-length training, suggesting the learning efficiency gains extend beyond convergence stability to overall sample efficiency.
\end{itemize}

\newpage

%===============================================================================
%                                   CONCLUSION
%===============================================================================
\section{Conclusion}
\label{sec:conclusion}

This section synthesizes the principal findings of this work, reflects on methodological successes and challenges, addresses threats to validity, and outlines promising directions for future research.

\subsection{Summary of Findings}

This project presented the 3D Game Generation AI Assistant, a comprehensive deep learning system integrating five components for voice-controlled 3D development in Blender:

\begin{enumerate}
    \item \textbf{VoxFormer Speech Recognition}: Achieves 2.8\% WER on LibriSpeech test-clean, surpassing established baselines (Wav2Vec 2.0: 3.4\%, Whisper-Base: 4.2\%) through the synergistic combination of WavLM pre-trained representations, Zipformer encoder architecture with Rotary Position Embeddings (RoPE), SwiGLU feed-forward networks, and hybrid CTC-attention training with curriculum learning. The architecture demonstrates that careful integration of modern transformer innovations yields substantial gains over conventional approaches.
    
    \item \textbf{Advanced Retrieval-Augmented Generation}: Achieves 0.92 faithfulness and 0.87 context precision through a novel 7-layer agentic architecture incorporating hybrid dense-sparse retrieval (BGE-M3 + BM25), Reciprocal Rank Fusion (RRF), cross-encoder reranking (MiniLM-L12), and iterative validation with automatic regeneration. This architecture addresses the hallucination problem inherent to large language models, achieving $<$5\% hallucination rate on domain-specific queries.
    
    \item \textbf{Text-to-Speech and Lip Synchronization}: Achieves 72ms time-to-first-byte with 4.14 Mean Opinion Score and 7.2 LSE-D lip sync error through integration of ElevenLabs Flash v2.5 neural TTS and SadTalker/MuseTalk audio-driven facial animation. The dual pipeline approach provides flexibility for different quality-latency requirements.
    
    \item \textbf{Digital Signal Processing Pipeline}: Achieves 20dB cumulative SNR improvement through a 6-stage pipeline incorporating MCRA noise estimation, MMSE-STSA enhancement, Deep Attractor Networks for source separation, and WPE dereverberation. This enables robust operation in realistic acoustic environments beyond controlled laboratory conditions.
    
    \item \textbf{Blender Model Context Protocol}: Achieves 95.6\% task success rate across 24 operations spanning object creation, transformation, modifier application, material assignment, animation keyframing, and file export. The MCP architecture enables natural language control of professional 3D software without scripting knowledge.
\end{enumerate}

\textbf{System-Level Impact}: User studies ($n=12$) demonstrate 71\% productivity improvement ($p < 0.001$) and SUS score of 81.4 (``Excellent'' rating), validating that the integrated system delivers meaningful workflow improvements beyond individual component benchmarks.

\subsection{What Worked Well}

Several design decisions proved particularly effective:

\begin{itemize}
    \item \textbf{Curriculum Learning}: Progressive training from 1-second to 30-second utterances improved VoxFormer convergence, reducing training iterations by 47\% and eliminating early divergence observed in fixed-length training.
    
    \item \textbf{Hybrid Retrieval Strategy}: Combining dense BGE-M3 embeddings with sparse BM25 retrieval improved both recall (+12\%) and faithfulness (+0.06), demonstrating that semantic and lexical signals provide complementary information for technical documentation.
    
    \item \textbf{Agentic Validation Loop}: The iterative faithfulness-checking mechanism reduced hallucinations from 12\% to under 5\%, addressing a critical limitation of single-pass RAG systems.
    
    \item \textbf{Modular Architecture}: The MCP-based system design enabled independent development, testing, and deployment of components, significantly reducing integration complexity during the 10-week development period.
    
    \item \textbf{Pre-trained Backbones}: Leveraging WavLM's self-supervised representations (trained on 94,000 hours) proved more effective than end-to-end training from scratch, confirming the value of transfer learning for speech recognition tasks.
\end{itemize}

\subsection{What Did Not Work}

Several initial approaches required revision:

\begin{itemize}
    \item \textbf{End-to-End Training Without Curriculum}: Initial attempts at training VoxFormer on full-length utterances from the start resulted in unstable convergence with loss oscillations and degraded final performance (4.1\% WER vs. 2.8\% with curriculum).
    
    \item \textbf{Dense-Only Retrieval}: Relying solely on BGE-M3 semantic embeddings missed important exact-match keywords in technical documentation. Terms like ``BSDF'' and ``Catmull-Clark'' have specific meanings that lexical retrieval captures more reliably.
    
    \item \textbf{Single-Pass Generation}: Initial RAG implementation without validation produced 12\% hallucination rates---unacceptable for providing accurate Blender guidance. The agentic validation loop was essential for production-quality responses.
    
    \item \textbf{Real-Time Lip Sync}: Initial attempts at real-time audio-to-animation with 60 FPS constraint produced visible artifacts. Switching to batch processing with brief buffering improved visual quality at acceptable latency cost.
\end{itemize}

\subsection{Threats to Validity}

We address potential validity threats and mitigation strategies:

\textbf{External Validity}: 
\begin{itemize}
    \item \textit{Threat}: User study with 12 participants may not generalize to the broader population of 3D artists.
    \item \textit{Mitigation}: Stratified sampling included 6 experts ($>$2 years Blender experience) and 6 novices ($<$6 months experience) with diverse backgrounds (game development, architectural visualization, animation).
\end{itemize}

\textbf{Construct Validity}: 
\begin{itemize}
    \item \textit{Threat}: WER measures transcription accuracy but may not fully capture real-world usability (e.g., command understanding).
    \item \textit{Mitigation}: Supplemented WER with task completion rate (95.6\%), user satisfaction scores (SUS), and end-to-end productivity metrics.
\end{itemize}

\textbf{Internal Validity}: 
\begin{itemize}
    \item \textit{Threat}: Performance may depend on specific hardware configurations.
    \item \textit{Mitigation}: Documented complete hardware/software specifications (Table~\ref{tab:hardware}) and provided Docker containerization for reproducibility.
\end{itemize}

\textbf{Statistical Conclusion Validity}: 
\begin{itemize}
    \item \textit{Threat}: Small sample sizes may limit statistical power.
    \item \textit{Mitigation}: Used paired comparisons where possible to reduce variance; reported effect sizes alongside $p$-values; acknowledged that larger studies would strengthen conclusions.
\end{itemize}

\subsection{Future Directions}

Several promising directions emerge from this work:

\begin{enumerate}
    \item \textbf{Model Distillation}: Apply knowledge distillation techniques \cite{hinton2015distilling} to create compact VoxFormer variants (20-30M parameters) suitable for edge/mobile deployment without significant accuracy degradation.
    
    \item \textbf{Streaming Inference}: Implement chunk-based processing with sliding windows for real-time STT, enabling sub-200ms transcription latency for responsive voice interfaces.
    
    \item \textbf{Automated Knowledge Updates}: Develop documentation crawlers with incremental embedding updates to maintain RAG knowledge currency across software versions automatically.
    
    \item \textbf{Multi-modal Understanding}: Integrate visual scene understanding (e.g., segment-anything models) to enable context-aware assistance based on current viewport contents (``move the selected object to the left'').
    
    \item \textbf{Procedural Generation}: Extend beyond retrieval and modification to AI-driven 3D asset creation using diffusion-based generators, enabling commands like ``generate a medieval castle with these reference images.''
    
    \item \textbf{Cross-Application Generalization}: Adapt the MCP architecture to other creative software (Maya, Unreal Engine, Unity) to validate generalizability of the voice-controlled assistant paradigm.
\end{enumerate}

\subsection{Final Remarks}

This work establishes a foundation for AI-assisted creative tools that understand natural language, access relevant knowledge bases, and execute complex operations in professional software environments. The integration of five deep learning components---speech recognition, retrieval-augmented generation, text-to-speech synthesis, digital signal processing, and automation protocols---demonstrates that comprehensive AI assistants require multiple specialized subsystems working in concert.

The 71\% productivity improvement and 81.4 SUS score demonstrate that voice-controlled interfaces can meaningfully democratize access to complex 3D development workflows. Perhaps most significantly, novice users showed higher satisfaction than experts, suggesting that natural language interfaces particularly benefit those without extensive software expertise.

As large language models continue to advance and multimodal understanding matures, such integrated AI assistants will become indispensable aids for creative professionals---enabling them to focus on artistic vision while delegating technical implementation to intelligent systems.

\newpage

%===============================================================================
%                                  REFERENCES
%===============================================================================
\printbibliography

\end{document}
