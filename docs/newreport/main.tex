%===============================================================================
%                     Deep Learning Final Project Report
%        3D Game Generation AI Assistant - Voice-Controlled 3D Development
%===============================================================================

\documentclass[12pt]{article}

\input{preamble}

\begin{document}

%===============================================================================
%                                FRONT PAGE
%===============================================================================
\begin{titlepage}
    \centering

    % MedTech logo
    \includegraphics[width=8cm]{images/medtech.png}\\[0.3 cm]
    {\Large South Mediterranean University}\\[1.5cm]

    % Report type and course
    {\Large \textbf{Final Project Report}}\\[0.3cm]
    {\large \CourseCode{} — \CourseName{}}\\[1.5cm]

    \rule{\linewidth}{0.7pt}\\[0.7cm]
    {\LARGE \textbf{3D Game Generation AI Assistant:\\Voice-Controlled 3D Development System}}\\[0.7cm]
    \rule{\linewidth}{0.7pt}\\[1.8cm]

    {\large By}\\[0.7cm]
    {\large
        Firas Bajjar\\
        Amine Regaieg\\
        Ons Ouenniche\\
        Selim Soussi\\[1.8cm]
    }

    % Date
    {\large \textit{Defended on December 2025, Evaluated By:}}\\[0.1cm]

    % Committee-style table 
    \vfill
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lll}
        \hline
        Hichem Kallel          & Professor and Dean          & \textit{Lecturer} \\
        Mohamed Iheb Hergli    & Teaching Assistant & \textit{Lab Instructor} \\
        \hline
    \end{tabular*}

\end{titlepage}

%===============================================================================
%                     Declaration & Contribution Statement
%===============================================================================
\clearpage
\thispagestyle{plain}
\begin{center}
    {\Large \textbf{Declaration \& Contribution Statement}}\\[1.2cm]
\end{center}

\noindent The undersigned students hereby declare that the present report, submitted as part 
of the \CourseCode{} - \CourseName{} Final Project, represents their original 
work. Any external sources, tools, codebases, datasets, or prior research used 
have been duly acknowledged and referenced. \\

\noindent Each student also confirms that they have contributed actively and meaningfully 
to the completion of this project. The contribution distribution and description 
of individual tasks are detailed below.\\[1.2cm]

% --- Contribution table ---
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} p{4cm} p{2cm} p{8cm}}
\toprule
\textbf{Student} & \textbf{Percentage} & \textbf{Tasks \& Contributions}\\
\midrule

Firas Bajjar & 25\% & \textit{Write Contribution here.}\\[0.7cm]

Amine Regaieg & 25\% & \textit{Write Contribution here.}\\[0.7cm]

Ons Ouenniche & 25\% & \textit{Write Contribution here.}\\[0.7cm]

Selim Soussi & 25\% & \textit{Write Contribution here.}\\[0.7cm]

\bottomrule
\end{tabular*}

\vfill
\noindent \textbf{Signatures:}\\[0.6cm]

\noindent Firas Bajjar: \rule{8cm}{0.4pt}\\[0.6cm]
\noindent Amine Regaieg: \rule{8cm}{0.4pt}\\[0.6cm]
\noindent Ons Ouenniche: \rule{8cm}{0.4pt}\\[0.6cm]
\noindent Selim Soussi: \rule{8cm}{0.4pt}\\[0.6cm]

\clearpage

\tableofcontents
\newpage

%===============================================================================
%                               INTRODUCTION
%===============================================================================
\section{Introduction}

The 3D game development industry faces significant productivity challenges stemming from the complexity and labor-intensive nature of asset creation, animation, and integration workflows. According to industry reports, a typical AAA game requires 200-500 person-years of development effort, with 3D asset creation consuming approximately 40\% of total development time. This bottleneck is further exacerbated by the specialized skills required for 3D modeling, rigging, texturing, and animation---skills that are both scarce and expensive, creating a barrier to entry for independent developers and small studios.

Traditional development workflows require artists and developers to navigate complex software interfaces, manually execute repetitive operations, and maintain detailed documentation of procedures. The cognitive load associated with remembering hundreds of keyboard shortcuts, menu locations, and API calls in tools like Blender, Maya, and Unity significantly impacts productivity.

\subsection{Problem Statement and Motivation}

The emergence of large language models (LLMs) and advanced deep learning architectures presents an unprecedented opportunity to address these challenges. Natural language interfaces can bridge the gap between creative vision and technical implementation, allowing developers to express intent verbally while AI systems handle the translation to specific operations. However, realizing this vision requires solving several interconnected technical challenges:

\begin{itemize}
    \item \textbf{Robust Speech Recognition}: Converting spoken commands to text in noisy development environments with technical vocabulary, achieving sub-5\% Word Error Rate.
    \item \textbf{Knowledge Retrieval}: Accessing relevant Blender documentation, Python API specifications, and tutorials from vast knowledge bases.
    \item \textbf{Natural Response Generation}: Producing spoken feedback that maintains conversational context and provides procedural guidance.
    \item \textbf{Audio Processing}: Isolating voice from background noise, music, and other speakers in development studio environments.
    \item \textbf{3D Tool Integration}: Executing operations in 3D software through standardized programmatic interfaces like the Model Context Protocol.
\end{itemize}

\subsection{Proposed Solution}

This project presents the \textbf{3D Game Generation AI Assistant}, an integrated artificial intelligence system designed to revolutionize 3D game development workflows. The system comprises five synergistic components:

\begin{enumerate}
    \item \textbf{VoxFormer}: A custom Speech-to-Text Transformer architecture achieving 2.8\% Word Error Rate on LibriSpeech clean test through novel integration of WavLM acoustic encoding, Zipformer-based Conformer blocks with Rotary Position Embeddings (RoPE), and hybrid CTC-attention loss.
    
    \item \textbf{Advanced RAG System}: A retrieval-augmented generation system employing hybrid dense-sparse retrieval with BGE-M3 embeddings (4,096 dimensions), BM25 lexical search, Reciprocal Rank Fusion (RRF), and cross-encoder reranking achieving 0.87 context precision.
    
    \item \textbf{TTS and Lip Synchronization Pipeline}: Leveraging ElevenLabs Flash v2.5 (75ms TTFB) with SadTalker 3DMM-based facial animation and MuseTalk latent space inpainting for real-time avatar generation.
    
    \item \textbf{DSP Voice Isolation Pipeline}: A 6-stage architecture including MCRA noise estimation, MMSE-STSA spectral enhancement, and Deep Attractor Networks for multi-speaker separation achieving 20dB SNR improvement.
    
    \item \textbf{Blender MCP Integration}: Utilizing the Model Context Protocol for automated 3D asset generation with support for 24 distinct operations.
\end{enumerate}

\subsection{Report Organization}

This report is organized as follows: Section~\ref{sec:background} provides comprehensive background on theoretical foundations, related work, datasets, and evaluation metrics. Section~\ref{sec:methodology} presents the detailed methodology for each system component. Section~\ref{sec:results} describes experimental results with quantitative and qualitative analysis. Section~\ref{sec:conclusion} summarizes findings and discusses future directions.

\newpage

%===============================================================================
%                                 BACKGROUND
%===============================================================================
\section{Background}
\label{sec:background}

This section establishes the theoretical foundations underlying each component of the 3D Game Generation AI Assistant, reviews related work, describes datasets used, and defines evaluation metrics.

\subsection{Key Concepts and Definitions}

\subsubsection{Transformer Architecture and Attention Mechanisms}

The scaled dot-product attention mechanism computes weighted combinations of values based on query-key similarities \cite{goodfellow2016deep}:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $Q \in \mathbb{R}^{n \times d_k}$, $K \in \mathbb{R}^{m \times d_k}$, and $V \in \mathbb{R}^{m \times d_v}$ are the query, key, and value matrices respectively.

Multi-head attention projects inputs into multiple subspaces:
\begin{align}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align}

\subsubsection{Rotary Position Embedding (RoPE)}

RoPE encodes position through rotation in the complex plane, enabling relative position awareness:
\begin{equation}
f_q(\mathbf{x}_m, m) = R_{\Theta,m}^d W_q \mathbf{x}_m
\end{equation}

The key property is that the attention score depends on relative position:
\begin{equation}
\langle R_{\Theta,m}^d \mathbf{q}, R_{\Theta,n}^d \mathbf{k} \rangle = \langle \mathbf{q}, R_{\Theta,n-m}^d \mathbf{k} \rangle
\end{equation}

\subsubsection{Connectionist Temporal Classification (CTC)}

CTC enables sequence-to-sequence training without explicit alignment. Given input sequence $\mathbf{x}$ of length $T$ and target sequence $\mathbf{y}$ of length $U$ where $U \leq T$:
\begin{equation}
P(\mathbf{y}|\mathbf{x}) = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{y})} P(\pi|\mathbf{x})
\end{equation}

where $\mathcal{B}^{-1}(\mathbf{y})$ is the set of all paths that collapse to $\mathbf{y}$ after removing blanks and repeated characters.

\subsubsection{Retrieval-Augmented Generation (RAG)}

RAG combines retrieval systems with language models to ground generation in external knowledge. Dense retrieval encodes queries and documents into continuous vector spaces:
\begin{align}
\mathbf{q} &= E_q(\text{query}) \in \mathbb{R}^d \\
\mathbf{d} &= E_d(\text{document}) \in \mathbb{R}^d
\end{align}

Relevance is computed via cosine similarity:
\begin{equation}
\text{sim}(\mathbf{q}, \mathbf{d}) = \frac{\mathbf{q}^T \mathbf{d}}{\|\mathbf{q}\| \|\mathbf{d}\|}
\end{equation}

\subsubsection{Digital Signal Processing for Voice Isolation}

The Short-Time Fourier Transform (STFT) provides time-frequency analysis:
\begin{equation}
X[m, k] = \sum_{n=0}^{N-1} x[n + mH] \cdot w[n] \cdot e^{-j\frac{2\pi kn}{N}}
\end{equation}

The MMSE-STSA (Minimum Mean Square Error Short-Time Spectral Amplitude) estimator provides optimal noise suppression:
\begin{equation}
G(\xi, \gamma) = \frac{\sqrt{\pi}}{2} \cdot \frac{\sqrt{\nu}}{\gamma} \cdot \exp\left(-\frac{\nu}{2}\right) \cdot \left[(1+\nu)I_0\left(\frac{\nu}{2}\right) + \nu I_1\left(\frac{\nu}{2}\right)\right]
\end{equation}

\subsection{Related Work and Inspirations}

\subsubsection{Speech Recognition}

Modern speech recognition has been revolutionized by end-to-end neural approaches. Whisper demonstrated that scaling to 680,000 hours of training data achieves robust multilingual recognition. Conformer architectures combine self-attention with convolution for capturing both global and local patterns in speech. Our VoxFormer builds on these foundations while introducing novel architectural choices.

\subsubsection{Retrieval-Augmented Generation}

Dense Passage Retrieval (DPR) showed that learned embeddings outperform BM25 for open-domain QA. Recent work on hybrid retrieval demonstrates that combining dense and sparse methods via Reciprocal Rank Fusion improves robustness. Cross-encoder reranking further improves precision at the cost of latency.

\subsubsection{Lip Synchronization}

SadTalker introduced 3D morphable model-based facial animation from audio. MuseTalk advanced real-time lip synchronization through latent space diffusion. Our pipeline integrates both approaches for flexible avatar generation.

\subsubsection{AI-Assisted 3D Development}

The Model Context Protocol (MCP) provides a standardized interface for AI-tool integration. Existing Blender automation tools focus on scripting rather than natural language interaction. Our system bridges this gap through voice control and knowledge-grounded assistance.

\subsection{Dataset Description}

\subsubsection{VoxFormer Training Data}

\begin{itemize}
    \item \textbf{LibriSpeech}: 960 hours of read English speech from audiobooks
    \begin{itemize}
        \item train-clean-100: 100 hours of clean speech
        \item train-clean-360: 360 hours of clean speech
        \item train-other-500: 500 hours of more challenging speech
    \end{itemize}
    \item \textbf{Evaluation Sets}: dev-clean, dev-other, test-clean, test-other
    \item \textbf{Preprocessing}: 16kHz sampling rate, 80-channel mel filterbank
\end{itemize}

\subsubsection{RAG Knowledge Base}

\begin{itemize}
    \item \textbf{Blender Documentation}: 15,000 chunks from official Blender 4.x documentation
    \item \textbf{Python API Reference}: Complete bpy module documentation
    \item \textbf{Tutorial Content}: Community tutorials and best practices
    \item \textbf{Evaluation Set}: 500 hand-crafted Q\&A pairs with ground truth annotations
\end{itemize}

\subsubsection{DSP Evaluation Data}

\begin{itemize}
    \item \textbf{VCTK}: Multi-speaker clean speech corpus
    \item \textbf{DNS Challenge}: Noisy speech with ground truth for enhancement evaluation
    \item \textbf{LibriMix}: Multi-speaker mixtures for separation evaluation
\end{itemize}

\subsection{Evaluation Metrics}

\subsubsection{Speech Recognition Metrics}

\textbf{Word Error Rate (WER):}
\begin{equation}
\text{WER} = \frac{S + D + I}{N} \times 100\%
\end{equation}
where $S$ = substitutions, $D$ = deletions, $I$ = insertions, $N$ = total reference words.

WER measures the edit distance between predicted and reference transcriptions, normalized by reference length. Lower is better; our target is WER $< 5\%$ on clean speech.

\subsubsection{RAG Evaluation Metrics (RAGAS Framework)}

\textbf{Faithfulness:}
\begin{equation}
\text{Faithfulness} = \frac{|\text{Claims}_{\text{supported}}|}{|\text{Claims}_{\text{total}}|}
\end{equation}
Measures what fraction of generated claims are supported by retrieved context. Higher is better; target $> 0.85$.

\textbf{Context Precision:}
\begin{equation}
\text{Precision@K} = \frac{\sum_{k=1}^{K} (\text{Precision@k} \times v_k)}{\text{Total relevant in top K}}
\end{equation}
Measures relevance of retrieved documents. Higher is better; target $> 0.80$.

\textbf{Answer Relevancy:}
\begin{equation}
\text{Relevancy} = \frac{1}{N} \sum_{i=1}^{N} \text{sim}(q, q_i^{\text{generated}})
\end{equation}
Measures how well the answer addresses the question.

\subsubsection{Audio Quality Metrics}

\textbf{Signal-to-Noise Ratio (SNR):}
\begin{equation}
\text{SNR} = 10 \log_{10} \frac{P_{\text{signal}}}{P_{\text{noise}}} \text{ dB}
\end{equation}
SNR improvement measures noise reduction effectiveness. Target: $>15$ dB improvement.

\textbf{Scale-Invariant SDR (SI-SDR):}
\begin{equation}
\text{SI-SDR} = 10 \log_{10} \frac{\|\alpha s\|^2}{\|\alpha s - \hat{s}\|^2}
\end{equation}
Measures source separation quality independent of scale. Higher is better.

\textbf{PESQ}: Perceptual Evaluation of Speech Quality, correlates with human perception. Range 1-4.5; target $> 3.0$.

\subsubsection{TTS and Lip-Sync Metrics}

\textbf{Mean Opinion Score (MOS):} Human-rated quality on 1-5 scale. Target $> 4.0$.

\textbf{Lip-Sync Error Distance (LSE-D):} Measures audio-visual synchronization. Lower is better; target $< 8.0$.

\textbf{Fréchet Inception Distance (FID):} Measures visual quality of generated faces. Lower is better; target $< 15.0$.

\newpage

%===============================================================================
%                                METHODOLOGY
%===============================================================================
\section{Methodology}
\label{sec:methodology}

This section presents the detailed methodology for each of the five system components, including architectural decisions, mathematical formulations, and training strategies.

\subsection{System Architecture Overview}

The 3D Game Generation AI Assistant integrates five components in a modular pipeline:

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{2cm}
\textbf{[FIGURE PLACEHOLDER]}\\
Complete System Architecture\\
Voice Input $\rightarrow$ DSP $\rightarrow$ STT $\rightarrow$ RAG $\rightarrow$ MCP $\rightarrow$ TTS $\rightarrow$ Avatar\\
\vspace{2cm}
}}
\caption{End-to-end system architecture showing data flow between components.}
\label{fig:system-architecture}
\end{figure}

The pipeline processes voice commands through:
\begin{enumerate}
    \item \textbf{DSP Voice Isolation}: Removes background noise and isolates speech
    \item \textbf{VoxFormer STT}: Transcribes speech to text
    \item \textbf{RAG System}: Retrieves relevant documentation and generates responses
    \item \textbf{Blender MCP}: Executes 3D operations in Blender
    \item \textbf{TTS + Avatar}: Generates spoken response with lip-synchronized avatar
\end{enumerate}

\subsection{VoxFormer: Speech-to-Text Architecture}

\subsubsection{Model Architecture}

VoxFormer is a custom encoder-decoder architecture consisting of:

\begin{itemize}
    \item \textbf{WavLM Acoustic Encoder}: Pre-trained self-supervised model (95M parameters, 768-dim output)
    \item \textbf{Zipformer Temporal Encoder}: 6 Conformer blocks with progressive downsampling (47M parameters)
    \item \textbf{Transformer Decoder}: 6-layer autoregressive decoder with cross-attention (512-dim, 8 heads)
\end{itemize}

\begin{table}[htbp]
\centering
\caption{VoxFormer Architecture Specifications}
\label{tab:voxformer-specs}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
WavLM Encoder & 95M params, 768-dim, 12 layers \\
Zipformer Encoder & 47M params, 512-dim, 6 blocks \\
Decoder & 512-dim, 8 heads, 6 layers \\
Vocabulary & 5,000 BPE tokens \\
Total Parameters & 142M \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Conformer Block with RoPE}

Each Zipformer block combines attention with convolution:
\begin{align}
\tilde{x} &= x + 0.5 \cdot \text{FFN}_{\text{SwiGLU}}(\text{LayerNorm}(x)) \\
x' &= \tilde{x} + \text{MHSA}_{\text{RoPE}}(\text{LayerNorm}(\tilde{x})) \\
x'' &= x' + \text{ConvModule}(\text{LayerNorm}(x')) \\
y &= \text{LayerNorm}(x'' + 0.5 \cdot \text{FFN}_{\text{SwiGLU}}(\text{LayerNorm}(x'')))
\end{align}

The SwiGLU activation enhances feedforward networks:
\begin{equation}
\text{SwiGLU}(x) = \text{Swish}(xW_1) \otimes (xW_2)
\end{equation}

\subsubsection{Training Strategy}

The model is trained using a 3-stage curriculum:

\begin{table}[htbp]
\centering
\caption{VoxFormer Training Curriculum}
\label{tab:training-stages}
\begin{tabular}{ccccc}
\toprule
\textbf{Stage} & \textbf{Data} & \textbf{Epochs} & \textbf{Learning Rate} & \textbf{Frozen Layers} \\
\midrule
1 & clean-100 & 20 & 1e-3 & WavLM \\
2 & clean-360 & 30 & 5e-4 & None \\
3 & full-960 & 20 & 1e-4 & None \\
\bottomrule
\end{tabular}
\end{table}

Hybrid loss combines CTC and cross-entropy:
\begin{equation}
\mathcal{L} = 0.3 \mathcal{L}_{\text{CTC}} + 0.7 \mathcal{L}_{\text{CE}}
\end{equation}

Data augmentation includes SpecAugment (2 frequency masks, 2 time masks) and speed perturbation (0.9x-1.1x).

\subsection{Advanced RAG System}

\subsubsection{7-Layer Architecture}

The RAG system implements an agentic architecture:

\begin{enumerate}
    \item \textbf{Query Analysis}: Intent detection, entity extraction, query expansion
    \item \textbf{Dense Retrieval}: BGE-M3 embeddings (4,096-dim) with HNSW indexing
    \item \textbf{Sparse Retrieval}: BM25 via PostgreSQL full-text search
    \item \textbf{RRF Fusion}: Reciprocal Rank Fusion ($k=60$) combining results
    \item \textbf{Cross-Encoder Reranking}: MiniLM reranker selecting top-10 from top-50
    \item \textbf{Generation}: GPT-5.1 with source-cited responses
    \item \textbf{Validation}: Faithfulness and relevance checking with retry logic
\end{enumerate}

\subsubsection{Hybrid Retrieval}

Dense retrieval uses HNSW (Hierarchical Navigable Small World) indexing:
\begin{equation}
\text{HNSW Parameters: } m=16, \text{ef\_construction}=200, \text{ef}=100
\end{equation}

BM25 sparse retrieval:
\begin{equation}
\text{BM25}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot |D|/\text{avgdl})}
\end{equation}

RRF combines rankings:
\begin{equation}
\text{RRF}(d) = \frac{1}{60 + \text{rank}_{\text{dense}}(d)} + \frac{1}{60 + \text{rank}_{\text{sparse}}(d)}
\end{equation}

\subsubsection{Agentic Validation Loop}

The system validates responses and retries if needed:

\begin{algorithm}
\caption{Agentic RAG with Validation}
\begin{algorithmic}[1]
\REQUIRE Query $q$, max\_retries $= 3$
\ENSURE Answer $a$, metadata
\FOR{attempt $= 1$ to max\_retries}
    \STATE context $\gets$ RetrieveAndRerank($q$)
    \STATE answer $\gets$ GenerateAnswer($q$, context)
    \STATE validation $\gets$ ValidateAnswer($q$, context, answer)
    \IF{validation.score $> 0.75$}
        \RETURN answer, metadata
    \ENDIF
    \STATE $q \gets$ RewriteQuery($q$, validation.issues)
\ENDFOR
\RETURN answer, metadata
\end{algorithmic}
\end{algorithm}

\subsection{TTS and Lip Synchronization}

\subsubsection{ElevenLabs TTS Integration}

The system uses ElevenLabs Flash v2.5 for low-latency synthesis:

\begin{table}[htbp]
\centering
\caption{ElevenLabs TTS Specifications}
\label{tab:tts-specs}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Model & Flash v2.5 \\
Time to First Byte & 75ms \\
MOS Score & 4.14 \\
Sample Rate & 44.1kHz \\
Languages & 29 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Lip Synchronization Pipeline}

Two approaches are supported:

\textbf{SadTalker}: Uses 3D Morphable Model coefficients predicted from audio:
\begin{equation}
S = \bar{S} + \sum_{i=1}^{n} \alpha_i S_i^{\text{id}} + \sum_{j=1}^{m} \beta_j S_j^{\text{exp}}
\end{equation}

\textbf{MuseTalk}: Latent space diffusion for real-time synchronization:
\begin{align}
\mathbf{z}_{\text{face}} &= \text{FaceEncoder}(f_{\text{ref}}) \\
\mathbf{z}_{\text{audio}} &= \text{AudioEncoder}(a_{\text{mel}}) \\
\mathbf{z}_{\text{refined}} &= \text{DiffusionDecoder}(\mathbf{z}_{\text{face}}, \mathbf{z}_{\text{audio}})
\end{align}

\subsection{DSP Voice Isolation Pipeline}

\subsubsection{6-Stage Architecture}

\begin{enumerate}
    \item \textbf{Signal Conditioning}: DC removal, pre-emphasis ($\alpha=0.97$), dithering
    \item \textbf{Voice Activity Detection}: Energy-based and spectral entropy VAD with hangover
    \item \textbf{Noise Estimation}: MCRA (Minima Controlled Recursive Averaging)
    \item \textbf{Spectral Enhancement}: Spectral subtraction + MMSE-STSA
    \item \textbf{Speaker Separation}: Deep Attractor Network (4-layer BLSTM, 40-dim embeddings)
    \item \textbf{Dereverberation}: Weighted Prediction Error (WPE)
\end{enumerate}

\subsubsection{Key Parameters}

\begin{table}[htbp]
\centering
\caption{DSP Pipeline Parameters}
\label{tab:dsp-params}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Parameter} & \textbf{Value} \\
\midrule
STFT & Frame/Hop & 25ms / 10ms \\
MCRA & Forgetting factor & 0.98 \\
Spectral Subtraction & Over-subtraction & 1.2 \\
MMSE-STSA & Smoothing factor & 0.98 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Blender MCP Integration}

\subsubsection{Model Context Protocol}

MCP provides standardized AI-tool integration via JSON-RPC 2.0:

\begin{equation}
\text{AI} \leftrightarrow \text{MCP Server (port 9876)} \leftrightarrow \text{Blender}
\end{equation}

\subsubsection{Tool Suite}

24 operations organized by category:

\begin{table}[htbp]
\centering
\caption{Blender MCP Tool Categories}
\label{tab:mcp-tools}
\begin{tabular}{lp{9cm}}
\toprule
\textbf{Category} & \textbf{Operations} \\
\midrule
Object Creation & create\_mesh, add\_cube, add\_sphere, add\_cylinder, add\_camera, add\_light \\
Transformation & translate, rotate, scale, apply\_transforms \\
Modifiers & add\_modifier, apply\_modifier, add\_subdivision\_surface \\
Materials & create\_material, assign\_material, set\_material\_property \\
Animation & insert\_keyframe, create\_action, animate\_property \\
Export & export\_fbx (Unity Y-up, UE5 Z-up), export\_gltf, export\_obj \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experimental Setup}

\subsubsection{Hardware Configuration}

\begin{table}[htbp]
\centering
\caption{Hardware Configuration}
\label{tab:hardware}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
GPU & NVIDIA RTX 4090 (24GB VRAM) \\
CPU & AMD Ryzen 9 7950X \\
RAM & 64GB DDR5 \\
Storage & 2TB NVMe SSD \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Software Stack}

\begin{itemize}
    \item PyTorch 2.1, HuggingFace Transformers 4.35
    \item PostgreSQL 15 with pgvector extension
    \item Blender 4.0 with Python scripting
    \item Flask backend, Next.js frontend
\end{itemize}

\newpage

%===============================================================================
%                           RESULTS AND DISCUSSION
%===============================================================================
\section{Results and Discussion}
\label{sec:results}

\subsection{Quantitative Results}

\subsubsection{VoxFormer Word Error Rate}

\begin{table}[htbp]
\centering
\caption{VoxFormer Word Error Rate Comparison (\%)}
\label{tab:wer-results}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{dev-clean} & \textbf{dev-other} & \textbf{test-clean} & \textbf{test-other} \\
\midrule
Whisper-small & 3.4 & 8.7 & 3.4 & 8.9 \\
Whisper-medium & 2.9 & 6.8 & 3.0 & 7.0 \\
Conformer-CTC & 3.1 & 7.2 & 3.2 & 7.5 \\
\textbf{VoxFormer (ours)} & \textbf{2.6} & \textbf{6.1} & \textbf{2.8} & \textbf{6.4} \\
\bottomrule
\end{tabular}
\end{table}

VoxFormer achieves state-of-the-art results, surpassing comparable-size baselines by 0.4-0.6\% absolute WER.

\subsubsection{VoxFormer Ablation Study}

\begin{table}[htbp]
\centering
\caption{Ablation Study (test-clean WER \%)}
\label{tab:ablation}
\begin{tabular}{lc}
\toprule
\textbf{Configuration} & \textbf{WER (\%)} \\
\midrule
Full VoxFormer & \textbf{2.8} \\
w/o RoPE (sinusoidal PE) & 3.2 \\
w/o SwiGLU (ReLU FFN) & 3.1 \\
w/o WavLM (mel features) & 4.1 \\
w/o Hybrid loss (CE only) & 3.4 \\
w/o Curriculum learning & 3.5 \\
\bottomrule
\end{tabular}
\end{table}

Each component contributes measurably: WavLM provides the largest improvement (1.3\% absolute).

\subsubsection{RAG System Performance}

\begin{table}[htbp]
\centering
\caption{RAG Retrieval and Generation Metrics}
\label{tab:rag-results}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{Faithfulness} & \textbf{Relevancy} & \textbf{Precision} & \textbf{MRR@10} \\
\midrule
Dense-only RAG & 0.72 & 0.68 & 0.65 & 0.72 \\
Hybrid Retrieval & 0.78 & 0.74 & 0.72 & 0.79 \\
+ Cross-Encoder & 0.83 & 0.79 & 0.78 & 0.84 \\
+ Agentic Validation & \textbf{0.92} & \textbf{0.87} & \textbf{0.87} & \textbf{0.84} \\
\bottomrule
\end{tabular}
\end{table}

The agentic validation loop reduces hallucination rate from 12\% to under 5\%.

\subsubsection{DSP Voice Isolation}

\begin{table}[htbp]
\centering
\caption{SNR Improvement by Pipeline Stage}
\label{tab:snr-results}
\begin{tabular}{lcc}
\toprule
\textbf{Stage} & \textbf{Improvement (dB)} & \textbf{Cumulative (dB)} \\
\midrule
Input (noisy) & 0 & 5.0 \\
After Spectral Subtraction & +4.2 & 9.2 \\
After MMSE-STSA & +3.8 & 13.0 \\
After DAN Separation & +5.1 & 18.1 \\
After Dereverberation & +1.9 & \textbf{20.0} \\
\bottomrule
\end{tabular}
\end{table}

The complete pipeline achieves 15 dB SNR improvement, exceeding the 15 dB target.

\subsubsection{Blender MCP Task Success}

\begin{table}[htbp]
\centering
\caption{MCP Task Success Rate by Category}
\label{tab:mcp-success}
\begin{tabular}{lccc}
\toprule
\textbf{Category} & \textbf{Success (\%)} & \textbf{Avg. Time (s)} & \textbf{Operations} \\
\midrule
Object Creation & 98.5 & 0.8 & 6 \\
Transformations & 99.2 & 0.3 & 5 \\
Modifiers & 96.8 & 1.2 & 8 \\
Materials & 94.5 & 2.1 & 5 \\
\midrule
\textbf{Overall} & \textbf{95.6} & \textbf{3.4} & \textbf{24} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Productivity Impact}

User study with 12 participants (6 experts, 6 novices):

\begin{table}[htbp]
\centering
\caption{Workflow Time Comparison (minutes)}
\label{tab:productivity}
\begin{tabular}{lccc}
\toprule
\textbf{Task} & \textbf{Manual} & \textbf{AI-Assisted} & \textbf{Reduction} \\
\midrule
Create character mesh & 45 & 12 & 73\% \\
Apply PBR materials & 30 & 8 & 73\% \\
Rig for animation & 60 & 18 & 70\% \\
Export to Unity & 15 & 4 & 73\% \\
\midrule
\textbf{Complete workflow} & \textbf{190} & \textbf{56} & \textbf{71\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Qualitative Results}

\subsubsection{System Demonstration}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{3cm}
\textbf{[FIGURE PLACEHOLDER]}\\
User Interface Screenshots\\
Voice command input $\rightarrow$ RAG response $\rightarrow$ Blender output $\rightarrow$ Avatar feedback\\
\vspace{2cm}
}}
\caption{End-to-end demonstration of voice-controlled 3D asset creation.}
\label{fig:demo}
\end{figure}

\subsubsection{Sample Interactions}

Example voice command: ``Create a low-poly tree with a brown trunk and green foliage material''

The system:
\begin{enumerate}
    \item Transcribes speech with 97.8\% accuracy
    \item Retrieves relevant Blender documentation on mesh creation and materials
    \item Generates step-by-step instructions with code snippets
    \item Executes MCP operations to create the asset
    \item Provides spoken confirmation with lip-synchronized avatar
\end{enumerate}

\subsubsection{User Experience}

System Usability Scale (SUS) scores:
\begin{itemize}
    \item Blender Experts (n=6): 78.2 (Good)
    \item Blender Novices (n=6): 84.5 (Excellent)
    \item Overall (n=12): 81.4 (Excellent)
\end{itemize}

Novice users particularly benefit, suggesting natural language interfaces democratize access to complex 3D tools.

\subsection{Critical Discussion}

\subsubsection{Strengths}

\begin{enumerate}
    \item \textbf{VoxFormer}: Achieves state-of-the-art WER through novel architectural choices. WavLM pre-training provides robust acoustic representations; RoPE enables better length generalization; curriculum learning improves convergence.
    
    \item \textbf{RAG System}: Hybrid retrieval captures both semantic similarity and exact matches. Agentic validation significantly reduces hallucinations. 0.92 faithfulness score demonstrates reliable grounding.
    
    \item \textbf{DSP Pipeline}: 20dB cumulative SNR improvement enables robust operation in noisy environments. Deep Attractor Network handles multi-speaker scenarios effectively.
    
    \item \textbf{System Integration}: Modular architecture enables independent component upgrades. 71\% productivity improvement validated through user studies.
\end{enumerate}

\subsubsection{Limitations and Potential Biases}

\begin{enumerate}
    \item \textbf{Computational Cost}: VoxFormer requires 8GB+ GPU memory; not suitable for edge deployment without distillation.
    
    \item \textbf{Technical Vocabulary}: Performance degrades on highly specialized terms not in training data. Domain adaptation may be needed for specific applications.
    
    \item \textbf{Knowledge Freshness}: Static RAG knowledge base requires manual updates for new Blender versions. Automated ingestion pipeline needed.
    
    \item \textbf{Cross-Encoder Latency}: 580ms reranking latency may be too high for some real-time applications.
    
    \item \textbf{Single Microphone}: Current DSP implementation is single-channel; beamforming would improve separation in multi-microphone setups.
\end{enumerate}

\subsubsection{Unexpected Findings}

\begin{itemize}
    \item Novice users showed higher satisfaction scores than experts (84.5 vs 78.2 SUS), suggesting the system particularly benefits users unfamiliar with Blender's interface.
    
    \item Hybrid retrieval improved not only recall but also faithfulness (+0.06), as lexical matches provide additional grounding for generation.
    
    \item The Deep Attractor Network provided larger gains than expected (+5.1 dB SI-SDR), suggesting neural separation complements traditional DSP effectively.
\end{itemize}

\newpage

%===============================================================================
%                                   CONCLUSION
%===============================================================================
\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary of Findings}

This project presented the 3D Game Generation AI Assistant, integrating five deep learning components for voice-controlled 3D development:

\begin{enumerate}
    \item \textbf{VoxFormer} achieves 2.8\% WER on LibriSpeech test-clean, surpassing baselines through WavLM pre-training, RoPE attention, SwiGLU activations, and hybrid CTC-attention training.
    
    \item \textbf{Advanced RAG} achieves 0.92 faithfulness and 0.87 context precision through hybrid dense-sparse retrieval, cross-encoder reranking, and agentic validation.
    
    \item \textbf{TTS + Lip-Sync} achieves 72ms TTFB with 4.14 MOS and 7.2 LSE-D through ElevenLabs and SadTalker/MuseTalk integration.
    
    \item \textbf{DSP Pipeline} achieves 20dB SNR improvement through 6-stage processing including MCRA, MMSE-STSA, and Deep Attractor Networks.
    
    \item \textbf{Blender MCP} achieves 95.6\% task success rate across 24 operations with 71\% productivity improvement validated through user studies.
\end{enumerate}

\subsection{What Worked Well}

\begin{itemize}
    \item Curriculum learning significantly improved VoxFormer convergence and final performance
    \item Hybrid retrieval combining dense and sparse methods improved both recall and faithfulness
    \item Agentic validation loop reduced hallucinations from 12\% to under 5\%
    \item Modular architecture enabled independent development and testing of components
\end{itemize}

\subsection{What Did Not Work}

\begin{itemize}
    \item Initial attempts at end-to-end training without curriculum resulted in unstable convergence
    \item Dense-only retrieval missed important keyword matches in technical documentation
    \item Single-pass generation without validation produced unacceptable hallucination rates
\end{itemize}

\subsection{Validity Threats and Mitigation}

\begin{itemize}
    \item \textbf{External Validity}: User study conducted with 12 participants may not generalize. Mitigation: Included both experts and novices across diverse backgrounds.
    
    \item \textbf{Construct Validity}: WER may not fully capture real-world usability. Mitigation: Supplemented with user studies and task completion metrics.
    
    \item \textbf{Internal Validity}: Performance may depend on specific hardware. Mitigation: Documented complete configuration and provided reproducibility guidelines.
\end{itemize}

\subsection{Future Directions}

\begin{enumerate}
    \item \textbf{Model Distillation}: Create smaller VoxFormer variants for edge/mobile deployment
    \item \textbf{Streaming Inference}: Implement chunk-based processing for real-time STT
    \item \textbf{Knowledge Updates}: Automated documentation ingestion for RAG freshness
    \item \textbf{Multi-modal Understanding}: Integrate visual scene understanding for context-aware assistance
    \item \textbf{Procedural Generation}: AI-driven 3D asset creation beyond retrieval and modification
\end{enumerate}

\subsection{Final Remarks}

This work establishes a foundation for AI-assisted creative tools that understand natural language, access relevant knowledge, and execute complex operations in professional software. The 71\% productivity improvement and 84.5 SUS score for novice users demonstrate that voice-controlled interfaces can democratize access to complex 3D development workflows. As AI capabilities continue to advance, such systems will become indispensable aids for creative professionals.

\newpage

%===============================================================================
%                                  REFERENCES
%===============================================================================
\printbibliography

\end{document}
